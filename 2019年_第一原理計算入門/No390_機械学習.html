　機械学習は人工知能（AI）の一分野です。<br />
-----------------------------------------------------------------------<br />
◆ テキスト<br />
・株式会社システム計画研究所「Pythonによる機械学習入門」オーム社<br />
　かなり良いテキストです。専門外の人間が、専門家が使っている用語ってどいういうことなの？とかカーネルトリックで何が良くなるの？っというのが分かるように付録に書いています。出来れば付録の執筆者によってさらにページ数を増やした書籍を読みたい気持ちになります。この著者は専門外の人間にも分かるように要点が書かれているので凄い人です。他のテキストだと、カーネルトリックは何が良くなったのか分からないけれども（表現を変えただけにしか読み取れないテキストがある）、このテキストでは何が良くなったのがより詳しく書かれている。何で他のテキストはこのように分かり易く書かないんだろう？　他のテキストへの理解が追い付かなくて自前でプログラム構築ができない。<br />
・株式会社フォワードネットワークら「実装ディープラーニング」オーム社<br />
　いくつか難しいところもあるが、OSやcudaのインストール方法も図付きで説明されており、第一原理計算の導入にも役立つ情報が記載されている。学習係数の更新のための具体的な数式も記述されている。<br />
-----------------------------------------------------------------------<br />
◆ お勧めのHP<br />
・日本語のpythonの解説（機械学習の記述有り。特におススメ）：<br />
　<a href="http://www.turbare.net/transl/scipy-lecture-notes/index.html">http://www.turbare.net/transl/scipy-lecture-notes/index.html</a><br />
・各種の問題に適応した手法を記載したマップ<br />
　<a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</a><br />
・機械学習のpythonとの出会い<br />
　<a href="http://www.kamishima.net/mlmpyja/">http://www.kamishima.net/mlmpyja/</a><br />
・PyconJP2014でのチュートリアル<br />
　<a href="https://www.slideshare.net/payashim/pyconjp-2014-opencv">https://www.slideshare.net/payashim/pyconjp-2014-opencv</a><br />
・Deep learning JP<br />
　<a href="http://deeplearning.jp/documents/">http://deeplearning.jp/documents/</a><br />
・Keras<br />
　<a href="https://keras.io/ja/">https://keras.io/ja/</a><br />
・KerasでVGG16を使う<br />
　<a href="http://aidiary.hatenablog.com/entry/20170104/1483535144">http://aidiary.hatenablog.com/entry/20170104/1483535144</a><br />
-----------------------------------------------------------------------<br />
◆ 機械学習<br />
<br />
◇ニューラルネットワーク<br />
　ニューラルネットワークとは、「ニューラル」というなの通り、生物の神経細胞（ニューロン/neuron）をモデル化した機械学習アルゴリズムです。1940年代にはこうしたモデルが提案され、1950年代には本格的に機械学習として研究されてきた、機械学習の世界ではかなりの古株なアルゴリズムです。<br />
　当初研究されたのは（単純）パーセプトロンと呼ばれる入力層・中間層・出力層の３層からなるネットワークでした。これは1960年代に大きなブームを引き起こしましたが、1969年に人工知能の父とも言うべき、マービン・ミンスキーが発表した著書『パーセプトロン』で、パーセプトロンは線形分離不可能な問題を原理的に解けないことが指摘され、第１期のブームは終息してしまいました。線形分離不可能な問題は論理演算における非排他的論理和も含まれる基本的な部分でしたので、期待が大きかっただけに幻滅も大きかったのでしょう。ただし、中間層を２層にすれば線形分離不可能な問題も解けますので、これは少々誤解という側面もあったと思われます。<br />
　ニューラルネットワークの層を多層にするアイディアはかなり早い時期からありましたが、（省略）誤差逆伝搬法（バックプロパゲーション）の研究を待たなくてはなりませんでした。誤差逆伝搬法により多層での学習が可能になり、ニューラルネットワークは再び注目を集めました。この時期、有意義な進歩はありましたし、郵便番号読み取りのような実用例も生まれましたが、「過学習が発生しやすい」点や「ハイパーパラメータの調整が非常に難しく職人芸のようになってしまう」といったことから、より調整が容易で性能も高いサポートベクターマシンなど、別の機械学習的手法に流行が移っていきました。（省略）<br />
　深層学習の直接の源流は、2006年にジェフリー・ヒントンが発表した積層自己符号化器（stacked autoencoder）を多層ニューラルネットワークの事前学習に使った研究と言われています。その後、学習の工夫とも言えるDropoutや活性化関数ReLUが発見されたことで、複数の隠れ層を持つニューラルネットワークを現実的な計算量で学習できるようになりました。（省略）<br />
　深層学習とは多層ニューラルネットワークを使った機械学習です。2012年のILSVRCで優勝したトロント大チームのネットワークAlexNetは8層でした。2014年優勝のGoogLeNetは22層、2015年優勝のResidual Netで152層、2016年の最新の研究では1000層あまりのネットワークを収束させたと発表されています。ネットワークの形も色々と提案されています。<br />
・最も素朴な形のニューラルネットワークは、入力層から出力層へ一方的にデータを流し、層の間のみ全ニューロンを結合するネットワークです。<br />
・人の視神経から発想を得た畳み込みニューラルネットワーク（convolutional neural network/CNN）。入力層から出力層への一方方向にデータを流し、層の間のみ結合する点は上と同じですが、結合方法に畳み込みを導入した点が重要です。上で説明した素朴なニューラルネットワークよりパラメータ数が減っており、多層にしても収束が容易になっています。画像処理によく使われ、成功してきたネットワークです。<br />
・時系列の影響も加味した学習を実現するため、層の間の流れにループを導入したRNN(recorrent neural network)。特に過去の情報の影響をコントロールするようにしたLSTM（Long short-term memory）がよく使われます。RNN系のネットワークは、言語のように前後の継続性が重要なデータや、動画のように時系列性を有するデータの分析によく利用されます。<br />
　現在でも新しいネットワークが次々と提案されています。今後どのようなネットワークが出てくるか楽しみです。<br />
　機械学習において、対象データの特徴を捉える「特徴量」の導入は極めて有効な手段です。（省略）深層学習の導入以前は、この特徴量の導入と調整は人手で行っていました。深層学習の登場でこの状況は一変しました。深層学習は学習データから特徴量そのものを学習できるようになったのです。<br />
　最適な特徴量は、課題ごとに異なります。深層学習は学習データから特徴量を取り出すための仕組みを獲得しますので、常に問題に適した前処理を行っていることになります。画像判別などの多くのタスクで、深層学習が既存の機械学習手法を上回ったのは、適切な特徴量を獲得できた点が大きく貢献しています。この特徴量の獲得は、機械学習の歴史におけるブレークスルーだと、と言う研究者もいるほどです。<br />
　ただし、深層学習ではデータのみから最適な特徴量を獲得するため、学習に膨大なデータが必要になるという欠点もあります。研究者の長年の研究を超える前処理を学習する必要があるわけですから、仕方がないところではあります。<br />
　深層学習を自分のパソコンで実際に使ってみるためのヒントを説明しましょう。深層学習は、他の機械学習手法より圧倒的に多くの計算量を必要とします。少しだけ試してみる程度なら通常のパソコンのCPUでも可能ですが、通常はNVIDIAのグラフィックカード（GeForce/Quadro）や数値計算用ハードウェアのTeslaを使います。<br />
　まずは、Googleが深層学習の説明のために公開しているWebアプリケーションがあります。こちらで遊んでみるだけでも、深層学習がどのような原理で動いているかを知って楽しむことができます。<br />
　<a href="http://playground.tensorflow.org/">http://playground.tensorflow.org/</a>　<br />
　パソコンで深層学習を扱うには、何らかのミドルウェアを使うことが一般的です。機械学習でscikit-learnを使うように、深層学習向けにはCaffe, Chainer, TensorFlow, CNTK, Teano, Torch, ...など、数多くのミドルウェアが公開されています。少し前まではCaffeがよく使われていましが、新興のChainer, TensorFlowも人気があります。筆者の経験上、プログラミングができる方が色々なネットワークを試したいという場合はChainerが最も扱いやすいと思います。何しろ国産なので日本語のメーリングリストに開発メンバーが直接コメントしています。<br />
　Chainerは pip install chainer でインストールできるはずですが、環境によってはsetuptoolsのエラーがでることがあります。その場合は、pip install --upgrade -I setuptools で解決できます。（省略）<br />
　Chainerとscikit-learnを連携させることも可能です。本書の執筆者も改良に関わっているxchainerやscikit-chainerといったモジュールを組み合わせてください。<br />
（省略）複雑巨大に見えるアルゴリズムも、根本的にはアフィン変換と活性化関数の繰り返しです。難しそうな見かけにめげず、基礎を固めて立ち向かうことが重要でしょう。[5]<br />
・&nbsp;パーセプトロンは後に多層化されます。多層パーセプトロンと区別する形で、当初提案されてきた中間層１層のパーセプトロンは単純パーセプトロンと呼ばれるようになりました。<br />
・ReLU（レル）は、y=a*x*u(x)でuはステップ関数です。簡単に言えば、x &lt; 0でyは0、x &gt;= 0でy=a*xの関数です。シグモイド関数だと値の変化がわずかになってしまって多層で収束させるのが難しいのに対し、こんな簡単な関数の方が多層にしたとき収束し易いので使われています。<br />
・dropoutは、ある層のノード（節）をランダムに0にする手法です（反復一回毎に行います）。こうすることで過学習を防ぎ、汎用的な機能を得ることができるようになります。<br />
<br />
◇ 多層化が容易になった理由[7]<br />
　シグモイド関数を活性化関数f(Z)とすると、その微分は下記のようになる。<br />
&nbsp;　f&#39;(Z) = f(Z)○(1-f(Z))<br />
ここで、記号「○」はアダマール積、行列の成分ごとの積。この最大値は0.25となる。3層のニューラルネットワークの場合には、最大値を取り続けても、0.25<sup>3</sup>≒0.016倍に薄まった誤差が伝搬される形になる。10層になると入力層側の誤差は0.25<sup>10</sup>≒0.000001倍に薄まってしまう。これが多層で発生する勾配消失問題の原因の一つです。シグモイド関数は、脳のシナプスの伝達を模倣したものといわれていますが、多層化には向いていなかったようです。ここで、活性化関数としてReLUを利用した場合を考えてみます。ReLUは定義域が正のときは恒等写像となり、微分はf&#39;(Z)=1になります。1は何回かけても1なので、入力層側への誤差は薄まることがありません。すなわち、ReLUは多層化に向いている活性化関数といえます。ドロップアウトのような優れたアルゴリズムや、活性化関数にReLUやLeak ReLUを使用することにより、比較的容易に多層ニューラルネットワークを作成することができるようになりました。このため事前学習は、最近ではほとんど用いられてなくなったようですが、VGG-19などは、11層のモデルをしっかり固めてから、その上にさらに層を積み増す形で多層化を実現しています。<br />
<br />
◇ パーセプトロンの収束定理[1]<br />
　学習データが正例と負例で線形分離可能ならば、必ず有限回の繰り返しで収束して分離平面を見つけることができる。<br />
・データが線形分離可能でないならば、パーセプトロン学習は収束しない<br />
・線形分離可能である場合でも、ステップ数が非常に大きくなることがある<br />
・線形分離可能でないような学習の例として、XOR関数（一本の直線で分けられない）やパリティ関数がある<br />
※ 幾何的特徴としてパーセプトロンが識別できるのは、オイラー数の関数のみである　＞　連結した図形も非連結の図形も同じオイラー数を持つ場合、パーセプトロンでは図形が連結しているかどうかを判断できない。<br />
※ ただし、中間層を2層にすれば線形分離不可能な問題も解ける。XORの論理回路をNOTとANDとORで書いてみれば分かる。中間層一層目にNOTとANDが実現するように、中間層2層目にORが成り立つようにすれば、XORは実現できる。<br />
<br />
◇ ディープラーニング（DL）の勃興[1]<br />
　一時期、バックプロパゲーションなどで隆盛を極めた感のあるニューラルネットワークですが、1990年代の半ばを過ぎると閉塞感が漂いはじめました。主な原因は以下のものでした。<br />
・バックプロパゲーションは隠れ層の層数が大きくなると効率的でなく、過学習を抑えられない<br />
・層数やノード数のパラメータをどのように設定すればいいかの指標がない<br />
その結果として理論的な研究が先走り過ぎて、真に有用な応用例が見つからなくなっていました。それに対してのブレークスルーが、今世紀の初頭ころから発表されました。それが層が深くても学習可能な深層学習（ディープランニング）です。その精力的な研究グループがCIFARであり、中心人物はジェフリー・ヒントンです。彼は、PDPの一員でもありました。そのときからニューラルネットワークの研究をやり続けていました。冬の時代もありましたが、捲土重来を期していたのです。最近のディープランニングの隆盛は、明らかに時機が味方しました。それは、<br />
・ビックデータ<br />
・GPUなどの計算資源<br />
です。こうして、過学習を起こさないだけの膨大な計算量とデータを確保することができるようになりました。逆に言えば、この二つが十分に確保できない限り深層学習が活用できないことに注意すべきである。<br />
※ 過学習＝訓練用のデータでは良い結果が得られるが、本番では良い結果が得られない。これは訓練用のデータが少なすぎるために起こる。過去2-3年の入試の答えを丸暗記しただけでは過去2-3年の問題が出ない入試で良い点が取れないのと同じ。<br />
※ 過学習は、少ないデータ数で複雑なモデル（機械学習はy=f(x)のf(x)を決める統計処理で、そのf(x)の式が複雑）を用いていることを示唆している（この他にも一般性が失われている（これを汎化性能が低下しているという）可能性も示唆される）。もし予測したい（物理現象などの）式が単純であると思われるならf(x)も単純なものになるように統計処理（機械学習）を適用したり、複雑なモデルを構築したいならデータ数を増やすことが必要になる。どうしてもデータ数が少ない状況で分析しなければならないなら、既に分かっている式の部分はデータから除いて分かってない部分を統計処理（機械学習）させる方がモデルはより単純になり少ないデータで解析できることになる。ちなみにこれは勾配ブースティング決定木とやっていることは同じ。<br />
<br />
◇ 階層型でのニューラルネットワーク[1]<br />
　x<sub>i,n+1</sub> = &Sigma;w<sub>i,j,n</sub>*f(x<sub>i,j,n</sub>)<br />
ここで、iとjは節（ニューラルネットワークの素子）の番号、nは層の番号、wは重みである。関数であるfはシグモイド関数が使われたりする。入力と出力、初期の重みを与えて、出力側から最急降下法によって重みを変えていく。<br />
※ 最急降下法は局所解に陥ることがある。<br />
※ 隠れ層＝中間層<br />
<br />
◇ 多階層ニューラルネットワークの学習[3]<br />
　ニューラルネットワークの学習手法である誤差逆伝搬法（バックプロパゲーション：最急降下法などを用いて目標とする値との差が小さくなるように係数を決める。出力層側から入力層側へと一層ずつ係数が計算されるので、目標とする値との差＝誤差、出力層側から入力層側へと係数を決める＝逆伝搬となるので、この名称が付けられていると私は思っている）は、原理的には多階層構造でもそのまま適用できます。そうすると、深層学習でも最初からニューラルネットワークを多階層で構成すれば、それで問題が解決するのではないかと思われるかもしれません。<br />
　しかし、一般に、階層が多いニューラルネットワークの学習には問題点があります。誤差逆伝搬法は、入力層に向かうにつれ修正量が少なくなります。多階層では入力側の重みはほとんど働かないということがわかっています。<br />
　この問題を解決する手法として、事前学習法（pre-training）が考案されました。誤差逆伝搬法を用いた教師あり学習を行う前に、何らかの方法で、重みの初期パラメータを適切なものに事前調整しておくというアイディアです。この事前学習は、入力の情報をなるべく失わないように、入力層側から1層ずつ順に教師なし学習でおこないます。正則化（大きな値をとる係数を減らすための工夫を正則化と呼ぶ）の効果で、入力層から上位に上がるにつれ活性化するノードの数は減るように学習されますので、うまく特徴となる情報を抽出しないと情報を保持することはできません。このプロセスで、もとの情報を保持しつつ、抽象度の高い情報表現を獲得していくことを、階層を重ねておこなうことが、深層学習のアイディアです。<br />
<br />
◇ ボルツマン・マシン[1]<br />
　ボルツマン・マシンは、1984年にヒントンらによって提案されたニューラルネットワークモデルです。このネットワークは階層型ではなく、相互結合型となっています。つまり、情報の流れは双方向的であり、各ユニットは出力を伝えた相手のユニットからも情報を受け取ります。このようなネットワークでは、自分の出力が別のユニットを経て、再び自分への入力として戻ってきます。これはフィードバック機構です。<br />
　ボルツマン・マシンでは、ある確率でネットワークの各状態が出現するような平衡状態に収束します。そして、平衡状態に収束してからも同じ動作規則で状態遷移を繰り返し、各状態の出現した回数の時間平均を計測すると、実現確率分布を求めることができます。この確率分布は、ボルツマン分布に一致することが示されています。<br />
<br />
◇ 焼き鈍し[1]<br />
※ ボルツマン・マシンでの確率分布がボルツマン分布に一致することから、エネルギーEと温度Tの二つのパラメーターで確率を示すことができます。<br />
　確率は温度Tが高いとエネルギーの小さな違いに鈍感になり、逆に温度Tが低いと敏感になります。温度T -&gt; 0の極限では最小エネルギーの状態の確率が1.0になり、ほかの状態の確率はすべて0になります。このことを上手く利用すると、ネットワークの状態をエネルギー関数の最小値に収束させることができます。<br />
　はじめは高い温度から出発してネットワークを動作させ平衡状態に到達させます。そのあとで平衡状態を崩さないように徐々に温度を下げていき、最終的に温度0の極限に到達させる方法が提案されました。この方法は、金属材料などを加熱し、徐々に冷却することで内部の欠陥を取り除く「焼き鈍し」に似ていることから「シミュレーションによる焼き鈍し」と呼ばれています。この際に重要なのが温度を下げるスケジュールです。あまり速い速度で温度を下げると探索が極小点に取り残されるからです。ギーマンらは、温度Tを対数的な条件に従って下げていけばネットワークの状態を必ず最小値に収束させられることを証明しました。しかし、実際には対数的温度変化はあまりにも穏やかなため、シミュレーションの時間が膨大になります。そこで、焼き鈍し法を用いるときにはしばしば最適性をある程度犠牲にして、より高速なスケジュールにします。<br />
<br />
◇ 制約付きボルツマン・マシン（RBM）[1]<br />
　ボルツマン・マシンは学習に必要な確率的推論に膨大な計算時間がかかるという問題が指摘されていました。制限付きボルツマン・マシンは、入力層と隠れ層の２層構造に単純化し、同一層内に結合がないようにしました。<br />
<br />
◇ 貪欲的層別事前学習（greedy layerwise pre-training）<br />
　毎回一つの階層レベルだけ教師なし学習する方法です。そして、以前に学習した構成を利用して次の層の学習を行います。原則的には、実行ごとにある層の重みを決定していくことになります。最終的に各層の重み情報が結合され、深い層のニューラルネットワークを構成します。これはあくまで前学習です。これが終わってから、教師つきでニューラルネットワークが学習されます。この手法は、従来困難であった深い層の学習の成功に大きく貢献しました。[3]<br />
　下記の「ボルツマン・マシンの学習アルゴリズム」が使われます。<br />
<br />
◇ ボルツマン・マシンの学習アルゴリズム[3]<br />
1) 重みw<sub>ij</sub>のsy基地をランダムに決定する<br />
2)&nbsp;可視ノード（データに対応する入出力のノード）を学習データにあわせて固定する。焼き鈍し法によって平衡状態を求める<br />
3) 平衡状態においてニューロンiとjが同時に発火する（1を出力する）確率p<sub>ji</sub><sup>+</sup>を求める<br />
4) 全てのニューロンを自由に動作させる。焼き鈍し法によって平衡状態を求める<br />
5) 平衡状態においてニューロンiとjが同時に発火する（1を出力する）確率p<sub>ij</sub><sup>-</sup>を求める<br />
6) 重みを&Delta;w<sub>ij</sub>=&eta;（p<sub>ji</sub><sup>+</sup> - p<sub>ij</sub><sup>-</sup>）に従って更新する<br />
7) 上記の2)から6)をネットワークのエネルギーが十分に低くなるまで繰り返す&nbsp;<br />
・学習期p<sub>&alpha;</sub><sup>+</sup>:可視層を学習データで固定したときに状態&alpha;が得られる確率<br />
・反学習期p<sub>&alpha;</sub><sup>-</sup>:全てのニューロンを自由に動作させたときに状態&alpha;が出現する確率<br />
　バックプロパゲーションの起源を精神分析する説があり、それから考え鵜rと反学習期は脳における夢に相当するかもしれません<br />
・G = &Sigma; p<sub>&alpha;</sub><sup>+ </sup>* ln(p<sub>&alpha;</sub><sup>+</sup>/p<sub>ij</sub><sup>-</sup>)を定義する。このGは、学習期と反学習期の確率分布が一致するときのみ0となり、それ以外では正の値をとる。この値が小さくなるように最急降下法で重みを更新する。<br />
・&eta;は学習のスピードを決める係数（学習係数）です。<br />
<br />
◇ AutoEncoder<br />
　ニューラルネットワークにおける内部表現の抽出については盛んに研究されています。もともとエンコーダ問題に対しては、圧縮率がそれほど高くないため、実用的に注目されていませんでした。しかし近年では、層の多いネットワークの事前学習法として注目されています。この手法は、自己符号化器（AutoEncoder）と呼ばれています。さらに、多層の階層的なネットワークで学習を再帰的に繰り返してデータを圧縮していく積層自己符号器（Stacked AutoEncoder）と呼ばれる手法も提案され、画像処理などさまざまな課題に応用されています。[1]<br />
　深層学習における初期パラメーターの調整で必要な、入力の情報をなるべく失わない、より少ないノードへの写像を学習する手段として、AutoEncoderとRestricted Bolzman Machine(RBM)がよく使われます。[3]<br />
　AutoEncoderでは、得られた中間層の値を新たな入力として、1階層上にずらして同様の表現学習を行います。この手順を積み重ねると、入力に近い側では単純な特徴が、階層が上がってゆくにつれて複雑な特徴が、それぞれ学習されます。[3]<br />
<br />
◇ 知能や本能には非理性的でニューラルネットの学習ではとらえられないふるまいもあります。[1]<br />
・自己欺瞞<br />
　自分で自分の心を欺くこと。自分の良心や本心に反しているのを知りながら、それを自分に対して無理に正当化すること。<br />
・誤帰属<br />
　ある刺激によって生じた理性的喚起の原因を、別の刺激に誤って帰属させること<br />
・認知的不協和<br />
　矛盾する認識をもったときに不快な感じ、この不快感を解消するために自分を納得させられるような理性的でない行動をすること<br />
などです。（省略）ニューラルネットのみでは説明が困難が学習もあります。ネズミは、何時間も経ってから吐き気を催させるような食べ物を避けるように学習します。こうした学習はたった一度の学習機会で獲得され、いったん身につくとなかなか消えません。この学習がニューラルネットであればそのネズミは死んでしまい、絶滅したかもしれません。これはディープラーニングの特徴とは対照的です。また吐き気の条件付けは動物がもつ固有の進化史に影響されます。たとえばネズミは視力が弱く、夜に餌を探すときには味覚や臭覚に頼っています。そのため、変な味のものにはすぐに嫌悪感をもたらしますが、見た目が変なものには嫌悪を学習しません。一方で、ウズラでは学習パターンは全く違っていて、嫌悪感が色と関連付けられやすいとされています。一つの説明は、学習の動機付けにかかわる内発的刺激を想定するものです。たとえば、扁桃体や側坐核の活性化により、海馬の長期増強が促進される。<br />
<br />
◇ 言語が本能である証拠（言語本能）として、ピジン言語のクレオール化や手話の獲得などの例が挙げられる。[1]<br />
　奴隷貿易時代などにおいて、言語や文化が異なる人々の間で、意思疎通の当座しのぎとして使われた混合語をピジン言語という。ピジン言語は単に単語を並べただけで、文法的な規則をほとんどもたない。それを母語として、子供の世代が複雑な文法を織り込んで作り出した表現力に富んだ新しい言語を「クレオール」と呼ぶ。アメリカ・ルイジアナ州やセーシェルにおける、フランス語をもとにしたクレオール言語が有名である。<br />
<br />
◇ 教師無し学習：特徴空間を使った分類でよく使われる。<br />
　データ　--＞　写像　--＞　特徴空間<br />
・データでの特徴抽出は何個であればよいかはいまのところ分からない。データと求める評価指標の関係がガウス分布になる様にすれば良い。[2]<br />
・境界は高次元から低次元に変換すれば低次元で曲線を描いて範囲を指定できる。サポートベクターマシーン（SVM）で良く用いられる。[2]<br />
<br />
◇ 半教師あり学習[2]<br />
　人工知能が判断が難しいと判定したものを人に判断してもらって精度を上げる。<br />
<br />
◇ 決定木[2]<br />
・決定木は情報エントロピーが小さくなるように分類していく。<br />
・自動で木を作る場合、木の深さは無理やりカットする。<br />
<br />
◇ 回帰木[2,8]<br />
・決定木での葉ノードに入力に対して関数が出力すべき数値が付随する。<br />
・物性値を議論するのにこのような回帰（regression）に関する分類器が使われる。<br />
※ 通常、回帰木は入力される数値の大小関係で目的とするデータとの誤差が小さくなるように出力する値とその範囲を決めている。そのため、範囲外となる（外挿）となる範囲は外挿に一番近い内挿の値となる（階段状で線が引かれ、範囲外は一定の値になることが文献[T1]のp.15を参照すれば理解できる）。ランダムフォレスト回帰などは良い相関（直線関係）が得られやすいため良い方法だと思われやすいが、こういった問題に対する補正がない回帰木を用いる場合は、内挿に近い外挿領域は他の手法より予測精度が低いことが想像できるのはわかるだろうか？　内挿領域だけを問題にするのであればランダムフォレスト回帰は強力ではあるが、「外挿を予測させて実際にデータを得て、そのデータを用いてさらに外挿を予測する」という一連の繰り返し処理を行う場合に不利であろうことは想像に難くない。そのため、回帰木でこのような繰り返し処理を行う場合やデータの外側にありそうな部分を少しでも予測しようという場合には、回帰木で問題がないのかを十分にチェックし、もし理論的な理由も用意できるのであれば用意しておいた方が良いだろう（例えば、範囲外の部分が指数関数的に値が減少していった裾にあたるところで一定の値としても目的の結果を得ることができるなど）。<br />
※ ランダムフォレストや勾配ブースティング決定木などが有名。実用上は、勾配ブースティング決定木が最も精度が良いが、ランダムフォレストが良い時もある。<br />
※&nbsp;勾配ブースティング決定木は、回帰木と入力の差をさらに次の回帰木で処理していくという形になっている。回帰木の深さ、ラーニングレート、連数などがパラメータとして挙げられるがまずはディフォルトで行ってみるとよい。回帰木の深さが精度に影響しやすい傾向。<br />
[T1] ランダムフォレスト回帰（https://www.slideshare.net/ssuser604c63/ss-93788451）<br />
<br />
◇&nbsp;サポートベクター回帰（SVR） [2]<br />
&nbsp; サポートベクター回帰（SVR）は、イメージとしては、誤差が一番小さくなるように直線で境界を引けるように座標を変換して、その後、境界線も含めて座標の変換を元に戻していると考えればよい。そのため曲線の境界線を引くことができる。実際の処理では、カーネル関数を用いてこれらと同等の処理を行えるようにしている。通常は2乗誤差を用いて境界線を引くが、ヒンジ関数を用いることで計算時間を短縮できる。<br />
[S1] サポートベクター回帰（https://datachemeng.com/supportvectorregression/）<br />
<br />
◇ ガウシアンプロセス回帰<br />
&nbsp; カーネル関数を決めるのが難しい。逆行列が計算に出てくるので計算が重い。精度を出すのが難しい。データのスケーリングが必要なこともある。スケーリングはたいていxでよいが、迷ったらyについてもスケーリングするようにする。<br />
<br />
◇ リッジ回帰 [2]<br />
&nbsp; 2乗誤差を用いた回帰の非線形化。二次の正則化項を付けることでパラメータ数が多すぎる問題を回避している。確率分布にもとづく回帰分析のモデルである「ガウシアンプロセス回帰」と結果はあまり変わらない。<br />
<br />
◇ 部分最小二乗回帰（PLS）[2]<br />
　目的変数と相関のある次元を探す線形回帰。カーネル法により非線形化が可能だが過学習しやすい。<br />
<br />
◇ 人工知能の導入[2]<br />
・人工知能系の仕事を請け負う企業・大学に丸投げは&times;<br />
　ノウハウが蓄積されない。高額に請求される。<br />
・人工知能系と仕事もできる部署を自社内に作る◎<br />
　解析・修正・運用がし易い。<br />
　仕事の難易度が分かるのでボラれない。<br />
・人工知能系専門の研究開発部署を自社内に作る△<br />
　優秀なスタッフを集めるのが難しい。<br />
・人工知能系の大学等での研究室との共同研究○<br />
　即戦力となる手法・人材の確保ができる。<br />
　時間の無い教授よりも、時間があって論文を読める大学院生の方が最先端の情報を持っている。<br />
<br />
◇&nbsp;強化学習 [3]<br />
　強化学習は、その設定上、これまでの教師あり/教師なし学習とは違う問題になります。ほかの機械学習手法との違いは、以下のようになります。<br />
・教師信号が間接的<br />
　「何が正解か」ではなく、時々報酬の形で与えられる<br />
・報酬が遅れて与えられる<br />
　例えば、将棋の勝敗、迷路のゴール<br />
・探求が可能<br />
　エージェントが自分で学習データの分布を変えられる<br />
・状態が確定的でない場合がある<br />
　確率分布でそれぞれの状態にいる確率を表す<br />
<br />
◇ 汎化性能[5]<br />
　機械学習の目的は学習データに含まれない未知のデータに対して有効な分類器を生成することです。学習データをテストデータに利用することは、出題されると知っている問題を丸暗記して試験で高得点を取るようなものです。テストでは高得点を取れても、はじめて見る問題を解けるようになったかは分かりません機械学習の目的は未知のデータを分類できるようになることですので、これでは困ります。この未知のデータに関する分類の能力は「汎化性能」と呼ばれ、分類器にとっては非常に重要な概念です。<br />
<br />
◇ 過学習[5]<br />
　汎化性能に関連する重要な概念として「過学習」があります。文字の通り、学習データにのみ適応しすぎて、かえって未知のデータへの当てはまりが悪くなった（汎化性能が下がった）状態のことです。<br />
<br />
◇&nbsp;分類問題[5]<br />
　データを分類する教師あり学習の問題です。学習データから分類基準を学習し、未知のデータに対して適切な分類ができるようになることを目指します。<br />
<br />
◇&nbsp;回帰問題[5]<br />
　数値を予測する教師あり学習の問題です。学習段階で入力と正解値との関係を学習し、未知の入力値に対しても適切な値を予測できるようになることを目指します。<br />
<br />
◇&nbsp;クラスタリング[5]<br />
　データからクラスタを作成する教師なし学習の問題です。正解がないところでも使用できますので、探索的なフェーズで使われることが多いです。データの前処理、中間処理でも利用されることがあります。<br />
<br />
◇ 応用例[5]<br />
　ここでは機械学習の代表的な応用例を紹介します。<br />
・迷惑メールフィルタ<br />
　現在のE-mailは迷惑メールが非常に多く、迷惑メールをフィルタリングする手段がないと実用的ではなくなってきています。配信元ドメインなどを指定してフィルタするだけでなく、機械学習による迷惑メールフィルタも数多く実装されています。迷惑メールフィルタはベイズ統計学を基礎とした実装が多いですが、実際の迷惑メールの傾向から判断基準を決めるという観点に立てば機械学習の応用例と言えるでしょう。<br />
・クレジットカードの不正検出<br />
　クレジットの利用履歴から不正利用を検知します。ルールも併用されているようですが、機械学習の手法も適用されています。<br />
・顔検出<br />
　デジタルカメラや各種ソフトウェアで実現されている顔検出ですが、ここでも機械学習が使用されることが多いです。<br />
・文字認識<br />
　ナンバープレートや郵便番号、より一般的には光学文字認識（Optical CharacterRecofnition/OCR）などの文字の認識でも機械学習が利用されています。ナンバープレートのようにある程度類型化されている対象であれば画像処理とパターンマッチングによる方法がとられることが一般的ですが、機械学習で実現している例も多数あります。<br />
・会話処理<br />
　Appleの「Siri」や日本マイクロソフトが開発している会話ボット「りんな」など、機械との対話が実用的になってきています。人工知能（AI）による恋愛相談も運用されています。<br />
・商品レコメンデーション<br />
　機械学習は、Amazonや各種ECサイトにみられるようなレコメンドシステムには欠かすことができない技術です。<br />
<br />
◇ 知能化技術の世代（世代の付け方は研究者によって異なる。[2]）<br />
第一世代：単純動作（プログラム通り動くだけ：論理、定理証明、自動販売機など）<br />
第二世代：探索・知識（手順に従って解を探す：知識工学、エキスパートシステムなど）<br />
第三世代：機械学習（処理や知識を自動獲得:統計的機械学習、深層学習など）<br />
第四世代：自動定式化（問題の解法を自立生成、最適化、探索、進化計算など）<br />
第五世代：感性理解（人とロボットの共生：感性脳情報科学など）<br />
第六世代：人工脳（脳と同等の機能の構築、高度知能、脳型計算など）<br />
<br />
◇人工知能の学会における研究分野[2]
<table border="1" cellpadding="1" cellspacing="1" style="width: 556.72px;">
	<tbody>
		<tr>
			<td style="width: 155.72px;">推論機構</td>
			<td style="width: 390.72px;">帰納推論、アブダクション、仮説推論、類推、事例ベース推論、モデルベース推論、メモリベース推論、定性推論、時間推論</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">機械学習</td>
			<td style="width: 390.72px;">概念形成、概念クラスタリング、知識精錬、計算論的学習、知識発見、データマイニング、遺伝的アルゴリズム、強化学習</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">プランニング</td>
			<td style="width: 390.72px;">プラン認識、即応プランニング</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">知識表現</td>
			<td style="width: 390.72px;">常識、不完全な知識、不確実性</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">自動化推論</td>
			<td style="width: 390.72px;">探索、定理証明、制約充足</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">論理プログラミング</td>
			<td style="width: 390.72px;">帰納論理プログラミング、制約論理プログラミング</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">知識ベース</td>
			<td style="width: 390.72px;">大規模知識データベース、オントロジ、知識の共有・再利用</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">自然言語処理</td>
			<td style="width: 390.72px;">機械翻訳、言語理解、対話システム</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">メディア理解</td>
			<td style="width: 390.72px;">画像理解、音声理解、マルチモーダル情報処理</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">知能ロボット</td>
			<td style="width: 390.72px;">ビエイビア・ベース・アーキテクチャ、アクティブセンシング、ロボット学習</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">人工ニューラルネット</td>
			<td style="width: 390.72px;">階層型ネット、相互結合型ネット、自己組織化マップ</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">分散AI</td>
			<td style="width: 390.72px;">マルチエージェントシステム、分散協調</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">知的エージェント</td>
			<td style="width: 390.72px;">ソフトウェアエージェント、インターネットエージェント</td>
		</tr>
		<tr>
			<td style="width: 155.72px;">応用システム</td>
			<td style="width: 390.72px;">知的CAI、人間-機械間インタラクション</td>
		</tr>
	</tbody>
</table>
<br />
◇ 機械学習の考え方の例 [2]<br />
・機械学習は、基本的には、事例を一般化する手法（内挿）であり、未知の現象を予測（外挿）するものではない<br />
・「分かっている（データにある）が気づいていないこと」を知る手法<br />
・人では把握しきれいない量のデータ、実行しきれない試行回数、耐え難い思考時間などを機械学習では行うことが可能<br />
※ 外挿領域についてはどうすればよいかは日夜研究が進められている。<br />
※&nbsp;機械学習は人間が決めたモデルy=f(x)でのfを求める統計処理（関数フィッティング）。囲碁では統計処理が人間に勝利した。文献[9]にも「大量の入力データxを用いてf(x)=yを満たす関数f(x)を決める」と書かれている。<br />
<br />
[1] 伊庭斉志「進化計算と深層学習」オーム社<br />
[2] Johokiko<br />
[3] 荒木雅弘「フリーソフトではじめる機械学習入門」森北出版株式会社（Weka利用法の記述有り）<br />
[4] 小高知宏「はじめての機械学習」オーム社（C言語での利用法の記述有り）<br />
[5] 株式会社システム計画研究所「Pythonによる機械学習入門」オーム社<br />
[6] 金森敬文「機械学習のための連続最適化」講談社（共役勾配法と最急降下法との結果の比較が記載されている）<br />
[7] 株式会社フォワードネットワークら「実装ディープラーニング」オーム社<br />
[8] 朱鷹の社wiki: <a href="http://ibisforest.org/index.php?%E5%9B%9E%E5%B8%B0%E6%9C%A8">http://ibisforest.org/index.php?%E5%9B%9E%E5%B8%B0%E6%9C%A8</a><br />
[9] 自己学習モンテカルロ法:&nbsp;<a href="https://ccse.jaea.go.jp/meeting/CCSE_WS30/PDF/Nagai.pdf">https://ccse.jaea.go.jp/meeting/CCSE_WS30/PDF/Nagai.pdf</a><br />
-----------------------------------------------------------------------<br />
◆ 機械学習の実際<br />
　機械学習のアルゴリズムには様々なものがありますが、いずれもベースとなる「アイディア」を反映した「数理モデル」のもとで「損失関数」を導出し、「最適化法」により損失関数を最小化する、です。<br />
<br />
◇ アルゴリズムのアイディアと最適化
<table border="1" cellpadding="1" cellspacing="1" style="width: 500px;">
	<tbody>
		<tr>
			<td style="width: 100.72px;">アルゴリズム</td>
			<td style="width: 91.72px;">アイディア</td>
			<td style="width: 102.72px;">数理モデル</td>
			<td style="width: 79.72px;">損失関数</td>
			<td style="width: 101.27px;">最適化法</td>
		</tr>
		<tr>
			<td style="width: 100.72px;">線形回帰</td>
			<td style="width: 91.72px;">特徴量の<br />
			重み付き平均</td>
			<td style="width: 102.72px;">アフィン変換</td>
			<td style="width: 79.72px;">二乗誤差</td>
			<td style="width: 101.27px;">解析解/勾配法</td>
		</tr>
		<tr>
			<td style="width: 100.72px;">ニューラルネットワーク</td>
			<td style="width: 91.72px;">ニューロンの発火</td>
			<td style="width: 102.72px;">アフィン変換+<br />
			活性化関数の組を多段化</td>
			<td style="width: 79.72px;">二乗誤差</td>
			<td style="width: 101.27px;">誤差逆伝搬法</td>
		</tr>
		<tr>
			<td style="width: 100.72px;">非線形SVM</td>
			<td style="width: 91.72px;">マージン最大化</td>
			<td style="width: 102.72px;">アフィン変換+<br />
			カーネル関数</td>
			<td style="width: 79.72px;">ヒンジ損失</td>
			<td style="width: 101.27px;">凸二次計画法</td>
		</tr>
	</tbody>
</table>
<br />
※得られた結果を再現するような多項式は数多く存在する。例えば、二次関数で済むような結果を三次や四次で描けば得られた結果に対応する入力値では問題はないが、他の入力では二次関数の値を再現できないことが起こる。機械学習を行う場合、大まかにでも、どのような数式になっているかを考えて結果を考察することが必要になる。画像処理のニューラルネットワークの場合は、各節（ノード）での結果が見れるものもあるため、どのような処理になっているかを検討するのもよいだろう。将来、画像以外の分野（熱電材料の分野などに適用）でも、どのような要素が結果に影響を与えているかをニューラルネットワークで検討することが出来る時代が来るのではないかと期待している。<br />
<br />
◇ 数式の理解<br />
・f(x) = Ax を線形変換と呼ぶ。<br />
・線形であるとは何か？<br />
　ある関数f(x)がxについて線形である」とは、<br />
　f(x+y) = f(x) + f(y)<br />
　f(&lambda;x) = &lambda;f(x)<br />
という条件が満たされていることを言います。&lambda;は任意のスカラーであり、xやyは「和とスカラー倍が定義されている何か」です。一方で、線形でない関数としては<br />
　f(x) = x<sup>2</sup><br />
　f(x) = sin(x)<br />
などがあります。線形でない非線形な関数は至る所に存在します。<br />
<br />
・アフィン変換<br />
　線形変換とよく似た変換として、アフィン変換というものがあります。アフィン変換は、<br />
　f(x) = Ax + b<br />
という式で表されます。アフィンとは平行移動のことです。線形変換との違いはまさしく平行移動の項bが入っている点です。これだけの違いですが、アフィン変換は非線形変換です。平行移動の項をバイアス項と呼び、バイアス項を足すことを「バイアスをかける」とも言います。<br />
　バイアス項が入ることで、変換後の空間はAの列空間とも異なる場合があります。bがAの列空間に含まれない場合、Aで表現できなかった方向へも点を動かすことができるからです。しかし、バイアス項は元のベクトルxに依存しない項なので、xの表現力が増えるわけではありません。単に異なる空間になるというだけです。バイアスは空間を見る視点を変えるための一種の調整項だと思ってください。<br />
----------<br />
　d次元の入力値xは、x &isin; R<sup>d</sup>と書くことができる。Rは実数（ということは複素数ではないということです）。<br />
　スカラーの観測値y = a<sup>t</sup>・x + b<br />
と書くとき、a &isin; R<sup>d</sup>、b &isin; Rとしたパラメーターにすれば関数の形を１つに決めることができる。tは転置を表す。パラメーターaとxは内積で計算しているためにスカラー値になっている。<br />
　この式の第１項はaについて線形ですが、式全体は非線形です。このような「線形変換+定数項」の変換をアフィン変換と呼びます。<br />
　d次元と難しく書いていますが、行列で書いていると思えば大学生のときの教科書やプログラミングでの配列の扱い方を調べて使えば良いことが分かります。<br />
----------<br />
<br />
・損失関数<br />
　ここでd次元の入力値xがm個あるとしましょう。m個あるのでそれを分けるためにxiと書きます（そのためiは1からmの値をとります）。ここでは簡単な下記の数式で考えてみます。<br />
　a<sup>t</sup>・x<sub>i </sub>+ b<br />
実際の値がyiとすると、その差は<br />
　&Sigma;<sub>i=1</sub><sup>m</sup> yi - (a<sup>t</sup>・x<sub>i </sub>+ b)<br />
となります。しかし、これでは正と負の値が同じ値だけ出てきたときに0に近い値になって差が小さくなったと勘違いしてしまいます。そこで正の値だけになる二乗和をとります。<br />
　L(a,b;x<sub>1</sub>,x<sub>2</sub>,...,x<sub>m</sub>,y<sub>1</sub>,y<sub>2</sub>,...,y<sub>m</sub>) = &Sigma;<sub>i=1</sub><sup>m</sup> yi - (a<sup>t</sup>・x<sub>i </sub>+ b)<sup>2</sup><br />
このLを損失関数と呼びます。Lを最小にするようなパラメーターaとbを探すということが機械学習でなされています。L(...;...)のように、セミコロンによる区切りを入れていることに注意してください。これは、Lはa,bを変数とする関数であることを示しています。セミコロンの右側の値は、実際に与えられた値の組という「条件」を示しています。「aとbの値を動かしたときにLの値がどうなるか」を考えるのであって、xとyの組は動かしがたい前提であるということです。<br />
<br />
・行列・ベクトル方程式による表現<br />
　Y=AD<br />
という単純な行列式を考えます。Aには入力値xが含まれていてパラメータは無し。Dはパラメータ（例えばaとb）のみで構成されています。行列での二乗は下記の様になります（ノルムと呼ばれます）。<br />
　L = （Y-AD）<sup>t</sup>（Y-AD）<br />
Lが最小となる条件を見つけるには<br />
　&part;L/&part;D = 0<br />
とすれえば良いことが分かります。この式を変形すると、<br />
　&part;L/&part;D = &part;[（Y-AD）<sup>t</sup>（Y-AD）] / &part;D<br />
　= &part;[（Y<sup>t</sup>Y - 2Y<sup>t </sup>AD + D<sup><font size="2">t</font></sup>A<sup><font size="2">t</font></sup>AD ）] / &part;D<br />
　= 2(-A<sup><font size="2">t</font></sup>Y + A<sup><font size="2">t</font></sup>AD) = 0<br />
となり、<br />
　A<sup><font size="2">t</font></sup>AD = A<sup><font size="2">t</font></sup>Y<br />
が得られます。A<sup><font size="2">t</font></sup>Aが可逆であれば、<br />
　D = (A<sup><font size="2">t</font></sup>A)<sup><font size="2">-1</font></sup>A<sup><font size="2">t</font></sup>Y<br />
と、最適なDの値を決めることができます。可逆であるためには、Aの列空間がDの列空間と一致する必要があります。<br />
（省略）連立方程式Y=AD、ここでY&isin;R<sup>m</sup>, A&isin;R<sup>mx(d+1), </sup>D&isin;R<sup>d+1</sup>でのD=(a, b)<sup><font size="2">t</font></sup>において、未知数a, bに対して条件が多すぎる場合、一般的にはこれをみたす解が存在する保証はありません。数学的には解が存在する場合もあります。ただし、その場合は他の行の線形和で書ける行がm-(d+1)本存在することになります。計測ノイズの影響で通常そのようなことは起こりませんし、起こったとしてもたくさんのデータのほとんどが無駄だったということに他なりません。あるデータの関係が他のデータの線形和で表現できてしまうということは、線形なモデルを考える限り、異なる情報を取得したことにはならないのです。Aの列空間がR<sup>d+1</sup>に一致しないのは、上記よりもさらに極端な場合で、他の行の線形和で書ける行がm-(d+1)本よりも多いときになります。<br />
・（解）＝（何らかの式）という形で表せる解を、解析解（analytic solution）や、閉じた形の解（closed form solution）と呼びます。行列を用いた解放は分かり易いですが、その適用範囲は極めて限られています。線形回帰に対する最小二乗法では式変形によって明示的に解を求めることができましたが、その他のほとんどのアルゴリズムでは解析解を得られません。たいていの場合、反復法によって最適化を行います。<br />
<br />
◇線形回帰の最小二乗解を考える<br />
　連立方程式　Y = AD<br />
　最小二乗解の満たすべき条件　(A<sup><font size="2">t</font></sup>A)D=A<sup><font size="2">t</font></sup>Y<br />
　最小二乗解　D = (A<sup><font size="2">t</font></sup>A)<sup><font size="2">-1</font></sup>A<sup><font size="2">t</font></sup>Y<br />
それでは、これらの意味を図形的に読み解いてみましょう。イメージしやすいよう、いったんd=1, m=3の場合、すなわち1変数のデータが3つある場合を考えます。まず、連立方程式Y=ADは「Aの列ベクトルの線形結合を取り、Yに一致するベクトルを作りたい」という欲求を表しています。ところが、Aの列空間は最も大きくてもR<sup>2</sup>です。言い換えれば、Aの列ベクトルでどのような線形結合をとっても、その点はある1つの平面上に位置することになります。したがって、Yが平面から外れた位置にある場合、Dをどう選ぼうとも線形結合ADをYに一致させることはできません。Yが平面に乗っている平面から外れているかはAの列によって決まります。つまり、我々の動かせるパラメータDによらず、データがすべて出そろった段階で決まっているのです。この時点で、誤差E=Y-ADをゼロベクトルにするDが存在するとは限らないということになります。この事実は、連立方程式の解が存在するとは限らないというこに対応しています。最小二乗法では、誤差Eを0にはできないまでも、そのノルムが最小になるような条件を掛けてDを定めたのでした。このときEはどのようなベクトルになっているのでしょうか。もちろんDを動かして実現しうる限り最小のベクトルになっているのですが、ノルムの等しいベクトルは無数に存在するため、その方向についても知っておきたいところです。<br />
　今一度、最小二乗法の式に立ち戻り、式を変形してみると、<br />
　A<sup><font size="2">t</font></sup>(Y-AD)= A<sup><font size="2">t</font></sup>E = 0<br />
という式を作ることができます。<br />
　この式が主張しているのは、L2ノルムが最小になるような誤差ベクトルは、データ行列Aの列空間に直交するということです。<br />
（省略）ADはもちろんAの列空間上にいます。一方で、YはAの列空間にいるとは限りません。そこで、Yを分解して<br />
　Y = Y<sub>//</sub> + Y<sub>&perp;</sub><br />
と書いてみます。Y<sub>//</sub>はAの列空間に収まる成分、Y<sub>&perp;</sub>は列空間に直交する成分です。「Aの列空間から外れる」ということを、「Aの列空間内でY<sub>//</sub>だけ進んでから、Aに直交するようにY<sub>&perp;</sub>だけ進む」と表現するわけです。このときA<sup>t</sup>Y<sub>&perp;</sub>=0ですから、<br />
　A<sup>t</sup>Y = A<sup>t</sup>(Y<sub>//</sub>+Y<sub>&perp;</sub>)=A<sup>t</sup>Y<sub>//</sub><br />
が成り立ちます。したがって、Y<sub>&perp;</sub>の情報は一切消えているのです。つまり、データから作れる出力値Yは、どうやってもAの列空間内にとどまってしまうのだから、Aの列空間に直交する成分Y<sub>&perp;</sub>については完全に度外視して、平面内で選びうる最良の値を取ろうということです。そして、度外視された方向の成分こそが誤差ベクトルEなのです。<br />
（省略）<br />
　2つのベクトルの内積の値は、大ざっぱに言って、成分のパターンが似通っていれば大きな値を持ちます。つまり、行列の各行は、それぞれの値と似ているベクトルを通し、似ていないベクトルを遮断するフィルタとしての働きを持っていることになります。あるクラスのベクトルに特徴的な成分のパターンがあるとすれば、それを行に持つ行列は、特定のクラスのデータを通し、他のクラスのデータを遮断するフィルタとして働くことになります。実際にはそこまで分かり易い状況にはなりませんが、例えば画像から作ったベクトルであれば輪郭を表すフィルタや向きを表すフィルタなど、より特徴らしい情報を取り出すためのフィルタを学習できると良いでしょう。その考えを推し進めていけば、フィルタは多ければ多いほど良さそうです。いくら多数のフィルタを掛けて大きなベクトルを作ったとしても、結果のベクトルは元のベクトルの分布よりも広い空間に位置することはできないものでした。したがって、元々線形分離不可能なデータ集合にどれだけフィルタを掛けても、線形分離不可能なままということになります。<br />
（省略）詳細な説明は割愛しますが、有限次元のベクトルxに対する線形変換はすべて行列を左から掛ける操作として表現できることが知られています。ベクトルに行列を掛ける操作には「空間の次元を上げられない」という制約があります。<br />
（省略）Aの列ベクトルの線形結合でできる空間をAの列空間と呼びます。同じく、行ベクトルで表現できる空間を行空間と呼びます。列空間と行空間の次元は一致することが知られています。<br />
（省略）逆行列を求めることと連立方程式を解くことは数学的には等価ですが、主に計算時間が異なります。逆行列を計算するための計算量はO(n<sup>3</sup>)であるのに対し、連立方程式を解くのに必要な計算量は（極端に高い精度を求めないならば）O(n<sup>2</sup>)で済むのです。両者は数式上は肩の整数が1つ違うだけですが、実感としては大きな差があります。計算量を考える際には、O(1), O(n), O(nlogn)は問題なし、O(n<sup>2</sup>)なら状況次第で可、O(n<sup>3</sup>)は著しく遅いのでやめておくべき、ぐらいに考えておくとよいでしょう。<br />
<br />
◇ ノルムと罰則項<br />
　通常、単にノルムと言えばL2ノルムを指します。これはd次元ベクトルxに対して<br />
｜｜x｜｜<sup>2</sup> = &radic;(&Sigma;<sub>i=1</sub><sup>d </sup>x<sub>i</sub><sup>2</sup>)<br />
と定義されます。<br />
　ノルムとは、大ざっぱに言えばベクトルの大きさのことです。L2ノルムの場合、上式から明らかな通り、成分の絶対値が大きければ大きいほどノルムも大きくなりますから、直感に沿うでしょう。<br />
　ノルムという概念の使い方は多岐にわたりますが、ここでは、罰則項への応用について説明します。<br />
　罰則項とは、あるデータセットから計算される損失関数に、パラメータ&omega;が「不自然」に当てはまらないよう制約を掛ける項です。<br />
　この罰則項付きの損失関数は<br />
　f<sub>p</sub>(&omega;)=f(&omega;)+C*p(&omega;)<br />
のような形をしています。Cは正の定数で、p(&omega;)が罰則項です。L2ノルムの場合は、<br />
　p(&omega;)=｜｜&omega;｜｜<sup>2</sup>です。<br />
このとき、罰則項付きの損失関数は、解のノルムが大きいとあたかも損失が大きいかのように振る舞うことになります。つまり、ノルムの大きな解は不自然だと主張しています。<br />
　パラメータの行がフィルタとして機能するには、データの特徴を表した「自然な」ベクトルであることが望ましいはずですが、自然なベクトルというのは往々にして、極端に大きなノルムは持ちません。<br />
　したがって、純粋に損失関数の谷底のみを目指すのではなく、ノルムについての経験的な知識を入れ込んだほうが「自然な」解を得られることが多いのです。罰則項を用いる場合も損失の形以外の変更はないので、通常の最適化問題として扱うことができます。<br />
　罰則項の導入は有効な手法ですが、注意点もあります。それはハイパーパラメータCの存在です。Cは罰則の強さを表し、大きすぎると元の損失関数が罰則項に覆い隠されてしまい、逆に小さすぎるとあまり効果が現れません。この値の決め方が問題になりますが、そもそも罰則項はアルゴリズムによって導かれるものではなく、「不自然な解はなるべく避けたい」という、人間の欲求によって付け加えられるものです。したがって、どの値が最適かというのは、目的に応じて人間がチューニングする必要があります。<br />
　各種の精度指標を利用して定めたり、計測データのS/N比に応じて定めたり、あるいはもっと洗練された手法を利用したりといくつかの判断基準はありますが、最終的にはケースバイケースで決めることになります。<br />
<br />
◇正則化(regularization)<br />
　罰則項を付けることを正則化とも言います。異常な解を避け、正常にするという意味です。L2ノルムを使った正則化はL2正則化、L1ノルムならL1正則化と呼ばれます。<br />
<br />
◇ ハイパーパラメータ<br />
　アルゴリズムによる決定対象にならず、ある種の勘や経験則、試行錯誤によって決定しなければならないパラメータのことをハイパーパラメータと呼びます。この説明は厳密な定義ではありませんが、大ざっぱにはこの程度の理解で十分です。重要なのは、ハイパーパラメータを決定するためにはいちいち最適化を実行し、その結果に応じて調整していかなければならないということです。したがって、大規模な問題になればなるほど、その調整には膨大な時間と手間を要することになります。そのためハイパーパラメータの個数は、アルゴリズムの使いやすさの重要な指標になります。<br />
　ここまでの話をまとめると、一般に最適化問題を解くためには、<br />
1. 評価関数<br />
2. 最適化アルゴリズム<br />
3. アルゴリズム固有のハイパーパラメータ<br />
という3つの要素を決める必要があると分かります。どの問題に対してどの組み合わせが最も有効かを決定する一般的な指針は、残念ながら存在しません。優れたアルゴリズムであっても適用対象や適用条件次第で無用の長物となりうるのです。<br />
<br />
◇ 多次元の理解<br />
　数式上は、1変数でも多変数でも形は全く同じです。一方で、他変数に対応する多次元空間での図形的な様子はイメージしづらく、通常は不可能です。とはいえ、予め2次元や3次元のケースを考えてイメージを作っておけば、厳密にどういうことかは分からなくても、おおよそ2次元や3次元のときと似たような事情だろうと推測することはできます。<br />
　このイメージが高次元においてどこまで妥当か判断するのは難しいですが、全く手がかりがないよりは良いでしょう。イメージできる「直交」という概念から、今重要視している数学的な性質を抜き出して、多次元にも使えるように拡張したということが大事なのです。<br />
　多次元の空間について考えるときは、2次元、3次元から出発してイメージをつかんでから、あえて1回それを捨て去って数式をガリガリと変形することに徹し、何が何だかわけが分からなくなったときに「これは2次元、3次元で言うと、どういうことだろう？」と立ち戻って考えることができるのが大事です。<br />
<br />
◇ 非線形の何が嬉しいのか？<br />
　あるベクトルの線形変換によって、元々のベクトルが持っている空間以上の広がりは表現できない、ということです。では、非線形変換ならばそれができるのでしょうか？<br />
　非線形とは、読んで字のごとく線形ではないものすべてのことです。したがって、それらすべてに共通する性質を見出すのは困難です。単純に非線形でさえあれば線形変換よりも便利に使えるというわけではなく、よく性質の知られた一部の非線形変換を利用することで、線形変換の限界を超えることができるようになります。そのような非線形変換の力をうまく使ったアルゴリズムとして、ニューラルネットワークと非線形SVMなどがあります。[1]<br />
　 ニューラルネットワークでは、活性化関数に非線形関数を用いる必要があります。これは言い換えると、活性化関数には線形関数を用いてはならない、ということです。なぜ線形関数を用いてはならないのでしょうか。それは、線形関数を用いると、ニューラルネットワークで層を深くすることの意味がなくなってしまうからです。線形関数の問題点は、どんなに層を深くしても、それと同じことを行う「隠れ層のないネットワーク」が必ず存在する、という事実に起因します。このことを具体的に（やや直感的に）理解するために、次の簡単な例を考えてみましょう。ここでは、線形関数であるh(x)=c*xを活性化関数として、y(x)=h(h(h(x)))を行う計算を3層のネットワークに対応させて考えることにします。この計算は、y(x)=c*c*c*xの掛け算を行いますが、同じことはy(x)=a*x（a=c<sup>3</sup>）の１回の掛け算で、つまり、隠れ層のないネットワークで表現できるのです。この例のように、線形関数を用いた場合、多層にすることの利点を生かすことができません。そのため、層を重ねることの恩恵を得るためには、活性化関数に非線形関数を使う必要があるのです。[2]<br />
<br />
・線形分離不可能<br />
　特徴ベクトルに対して線形変換を繰り返す限り、絶対に異なるクラスのデータを分離できないような分布。<br />
<br />
◇線形SVMと非線形SVM、そしてカーネル関数<br />
　さて、まず線形SVMの判別器としての式は、<br />
　y=&omega;<sup>t</sup>x+b<br />
です。yの符号がxの属するクラスに対応します。また、SVMは2クラス判別を基本とするので、&omega;は1本のベクトル（=1本の直線）です。線形SVMは関数としてはただのアフィン変換であり、パラメータを定める基準としてマージン最大化という問題を解いていることに特色があるのです。<br />
　一方、非線形SVMの判別器の式は<br />
　y=&Sigma;<sub>i=1</sub><sup>m </sup>&alpha;<sub>i</sub>K(x<sub>i</sub>,x) + b<br />
です。変わったのは第1項のみです(ただし、パラメータは&omega;<sub>i</sub>ではなく、サンプルの個数分だけ存在するスカラー値&alpha;<sub>i</sub>です）。ベクトルxの成分が、xの関数K(x<sub>i</sub>,x)に置き換わっただけともみなせます。このKをカーネル関数と呼びます。カーネルを使った表現の出所を見てみましょう。非線形SVMを始めとするカーネル法の仲間たちは、線形変換と非線形変換の折衷案のような方法を利用します。すなわち、<br />
　y = u<sup>t</sup>&phi;(x)+b<br />
という式を使います。ここで&phi;(x)は、学習対象となるパラメータを持たない非線形関数で、かつ、元のベクトルx&isin;R<sup>d</sup>よりも高次元のベクトルです。つまり、第1項は非線形関数の線形結合です。&phi;(x)自身はxについて非線形ですが、式全体はuおよび&phi;(x)について線形なのです。学習パラメータuについて線形なので、uについての学習はここまでの枠組みで実行できます。しかし、このままの形ではxより高次元かつ非線形なベクトルの内積を直接計算する必要があり、計算コストが増大してしまいます。非線形SVMでは、カーネルトリックと呼ばれる手法によってそのコストを抑えます。説明は割愛しますが、&alpha;<sub>i</sub>については、<br />
　u=&Sigma;&alpha;<sub>i</sub>&phi;(x<sub>i</sub>)<br />
が成り立つという定理があります。最適な重みは、代表的な点の値を使って表現できるという意味です。これを[y = u<sup>t</sup>&phi;(x)+b]の式に代入すると<br />
　y = u<sup>t</sup>&phi;(x)+b<br />
　　=&Sigma;&alpha;<sub>i</sub>&phi;(x<sub>i</sub>)<sup>t</sup>&phi;(x)+b<br />
となります。高次元の関数&phi;(x)が内積、すなわちスカラーの形に置き換えられました。カーネルトリックとは、この（高次元、高コストな）内積計算を省略する技法です。すなわち、<br />
　&phi;(x<sub>i</sub>)<sup>t</sup>&phi;(x) = K(x<sub>i</sub>,x)<br />
と、内積を2変数のスカラー値関数K(x,y)で置き換えてしまうのです。ずいぶん乱暴な気がしますが、Kがある種の性質を持つならば、このような置き換えが許されるということが証明されています（再生性と言います）。<br />
　具体的にカーネルとして使える関数は例えば、<br />
　K(x,y)=exp(-｜｜x-y｜｜<sup>2</sup>/(2&sigma;<sup>2</sup>))<br />
などです（&sigma;はハイパーパラメーター）。<br />
まとめると、ここまで重要視してきた、<br />
・特徴空間<br />
&rarr;高次元への非線形変換<br />
&rarr;判別に必要な1次元空間<br />
という手続きが<br />
・特徴空間<br />
&rarr;（高次元空間での内積をカーネル経由で省略）<br />
&rarr;判別に必要な1次元空間<br />
というふうに、高次元空間での操作を背後に隠す形で簡略化できるのです。何だか化かされたような感じですが、そういうものだと思ってください。<br />
　ニューラルネットワークに代わって一時代を築いた非線形SVMですが、やはりその根幹は「いかにして高次元空間への非線形変換を行うか」という点にあるのです。<br />
※ 線形SVMでは多次元であれば数多くの分類ができます。<br />
※ 非線形SVMでは無理やり次元を上げてデータを区切っています。例えば、XとYだけからなるデータで、データが円形に分布していれば、Z=X<sup>2</sup>+Y<sup>2</sup>として無理やりZ軸を追加することでデータが分類できることがあります。このように次元を上げることは線形変換ではできず、非線形変換で可能になります。<br />
<br />
◆ パラメーターの更新（学習係数の更新）<br />
----------<br />
◇最急降下法の更新則（確率的勾配降下法（stochastic gradient descent; SGD））<br />
　ある初期値&omega;<sub>0</sub>のもと、パラメーターの更新をt回繰り返したとします。このとき、次の更新値（t+1番目の値）を<br />
　&omega;<sub>t+1</sub> = &omega;<sub>t</sub> - &alpha;&nabla;f(&omega;<sub>t</sub>)<br />
と定めます。ここで&alpha;はパラメータ（学習率と呼ばれる）、ポテンシャルを下る方向が-&nabla;f(&omega;<sub>t</sub>)。これが最急降下法の更新則です。更新は、適切な基準を満たすまで繰り返します。例えば、評価関数の値が十分に小さくなったとき、更新量が十分小さくなったとき、あるいはもっと単純に一定回数繰り返したときなどに終了するといった調子です。<br />
<br />
◇モーメント法<br />
　最急降下法の更新則を修正して、<br />
　&Delta;&omega;<sub>t</sub> = - &alpha;&nabla;f(&omega;<sub>t</sub>) + &beta;&Delta;&omega;<sub>t-1</sub><br />
と置き換えます。&beta;はモーメントで、学習率&alpha;と同じく小さな値を持つ定数です。ある点での勾配が0でも、直前までの「慣性（モーメント）」が考慮されて止まらないというわけです。このように、一度促された勾配が減衰しながらも足され続けるため、ある点の勾配のみならず過去の勾配の情報も利用してポテンシャルの山を下ることができます。更新量に大域的な情報を入れ込む方法は他にも数多く提案されていますが、概ね勾配に基づいた更新則に「ちょっとした修正」を加えているものです。それだけ最急降下法は重要な基礎になっているのです。<br />
<br />
◇AdaGrad<br />
　ニューラルネットワークの学習では、学習係数の値が重要になります。学習係数が小さすぎると学習に時間がかかりすぎてしまい、逆に大きすぎると発散して正しい学習が行えません。この学習係数に関する有効なテクニックとして、学習係数の減衰（learning rate decay）という方法があります。これは、学習が進むにつれて学習係数を小さくするという方法です。最初は大きく学習し、次第に小さく学習するという手法で、ニューラルネットワークの学習では実際によく使われます。学習係数を徐々に下げていくというアイディアは、パラメータ「全体」の学習係数の値を一括して下げることに相当します。これをさらに発展させたのがAdaGradです。AdaGradは、一つひとつのパラメータに対して、オーダーメイドの値をこしらえます。AdaGradは、パラメータの要素ごとに適応的に学習係数を調整しながら学習を行う手法です。<br />
　h &larr; h + &part;L/&part;W （・） &part;L/&part;W<br />
　W &larr; W - &eta; * (1/&radic;h) * &part;L/&part;W<br />
ここで、Wは更新する重みパラメータ、&part;L/&part;WはWに関する損失関数の勾配、&eta;は学習係数を表します。（・）は行列の要素ごとの掛け算を意味しています。hはこれまで経験した勾配の値を2乗和として保持します。そして、パラメータの更新の際に、1/&radic;hを乗算することで、学習のスケールを調整します。これは、パラメータの要素の中でよく動いた（大きく更新された）要素は、学習係数が小さくなることを意味します。つまり、よく動いたパラメータの学習係数は次第に小さくなるという学習係数の減衰を、パラメータの要素ごとに行うことができるのです。AdaGradは、過去の勾配を2乗和としてすべて記録します。そのため、学習を進めれば進めるほど、更新度合いは小さくなります。実際のところ、無限に学習を行ったとすると、更新量は0になり、まったく更新されません。この問題を改善した手法として、RMSPropという方法があります。<br />
<br />
◇ RMSProp<br />
　RMSPropという手法は、過去の全ての勾配を均一に加算していくのではなく、過去の勾配を徐々に忘れて、新しい勾配の情報が大きく反映されるように加算します。専門的には「指数移動平均」と言って、指数関数的に過去の勾配のスケールを減少させます。<br />
<br />
◇ Adam<br />
　Adamは2015年に提案された新しい手法です。その理論はやや複雑ですが、直感的にはモーメント法とAdaGradを融合したものです。2つの手法の利点を組み合わせることで、効率的にパラメータ空間を探索することが期待できます。また、ハイパーパラメータの「バイアス補正（偏り補正）」が行われています。<br />
　Adamは3つのハイパーパラメータを設定します。ひとつは、これまでの学習係数（論文では&alpha;として登場）。後の2つは、<br />
一次モーメント用の係数&beta;1と二次モーメント用の係数&beta;2です。論文によると、標準の設定値は、&beta;1は0.9、&beta;2は0.999であり、その設定値であれば、多くの場合うまくいくようです。<br />
<br />
　SGD, Momentum（モーメント法）、AdaGrad, RMSProp, Adamと5つの手法を見てきましたが、どれを用いたら良いのでしょうか？ 残念ながら、すべての問題で優れた手法というのは（今のところ）ありません。それぞれに特徴があり、得意な問題、不得意な問題があります。<br />
　多くの研究では今でもSGDが使われています。最近では、多くの研究者や技術者がAdamを好んで使っているようです。<br />
----------<br />
<br />
◇オンライン学習<br />
　入力が与えられるごとに少しずつ学習を進めるような学習方法<br />
◇バッチ学習<br />
　全ての学習用データに対して重みの修正量を計算した後に、その総和を取ってまとめて修正を行うような学習方法<br />
<br />
◇ミニバッチ法<br />
　適当な個数（バッチサイズ）だけデータ（ミニバッチ）を取ってきます。そして、i番目のミニバッチB<sub>i</sub>の要素x<sub>j</sub>について計算した評価関数の勾配&nabla;f(&omega;;x<sub>j</sub>)を単に足し合わせた<br />
　&nabla;f<sub>i</sub>(&omega;) = &Sigma;<sub>xj&isin;Bi</sub> &nabla;f(&omega;;x<sub>j</sub>)<br />
という値を、更新則の&nabla;fに代入して重みを更新します。同じことをミニバッチに選ばれたデータがなくなるまで繰り返します。この繰り返し1回をエポックと呼び、n回目という意味でnエポックと数えます。繰り返しは予め定めたエポック数に達するまで続けます。単純な方法ではありますが、バッチサイズとエポック数が適切に選ばれていれば十分良い結果に到達できることが経験的に知られています。以上のように、勾配に基づいた更新則をミニバッチ単位で適用し、反復的に評価関数を最小化していく方法を総称して、確率的勾配降下法と呼びます。英語でStochastic Gradient Descentと言いますが、これを略したSGDという用語もよく用いられます。大規模データを利用する機械学習では、非常に多くの文脈でこのSGDが現れてきますので、是非覚えておいてください。<br />
（省略）ミニバッチの取り方はエポックごとにランダムに決めることが多いですが、データの性質次第では固定順でもそれなりにうまくいくこともあります。<br />
<br />
◇調和平均<br />
　n/(1/a<sub>1</sub> + 1/a<sub>2</sub> + ... + 1/a<sub>n</sub>)<br />
速度のように２つの値（速度の場合は距離と時間）の比で表せる数値の平均には、調和平均を使うことになる。<br />
<br />
◇幾何平均<br />
　<sup>n</sup>&radic;(a<sub>1</sub>a<sub>2</sub>...a<sub>n</sub>)<br />
幾何平均は金利計算のように積を累積させる場面で使われます。<br />
　ちなみに、算術平均は相加平均、幾何平均は相乗平均とも呼ばれます。<br />
<br />
◆ ベイズ統計[4]<br />
----------<br />
◇ 事象の種類<br />
・空事象：空集合0で表される事象を空事象という。<br />
・和事象：2つの事象A, Bについて、事象Aまたは事象Bが起こる事象を事象Aと事象Bの和事象といい、A&cup;Bで表す。<br />
・積事象：2つの事象A, Bについて、事象Aと事象Bが共に起こる事象を事象Aと事象Bの積事象といい、A&cap;Bで表す。<br />
・排反事象：2つの事象A, Bが同時に起こることがないとき事象Aと事象Bは互いに排反であるという。<br />
・余事象：事象Aに対して、事象Aが起こらないという事象を事象Aの余事象といい、A（頭にバー&quot;-&quot;を付ける）で表す。<br />
・事象Aの確率=事象Aの起こる場合の数/起こりうるすべての場合の数事象Aの確率をPr(A)で表す。<br />
<br />
◇大学数学での確率の定義<br />
（&Omega;, F）&nbsp;を可測空間とするとき、Fを定義域とする実数値関数Prが次の性質を満たすならばPrは（&Omega;, F）上の確率という。<br />
a) 0 =&lt; Pr(A) =&lt; 1,&nbsp; &forall;A &isin; F<br />
b) Pr(&Omega;)=1<br />
c) {An}が互いに素なFの元の列であれば、すなわち、<br />
An &isin; F, &forall;n &gt;= 1, An &cap; Ak = 0, n&ne;k<br />
ならば、Pr(&cup;An)=&Sigma;Pr(An)<br />
&nbsp;実際何をもって、確率の正確な定義とすべきかという問題は、昔から盛んに議論が行われ、今日でも諸学者の間で意見の一致が得られているとはいい難い。<br />
<br />
◇確率の基本性質<br />
・0=&lt;Pr(A)=&lt;1<br />
・Pr(A)=1<br />
・Pr(0)=0<br />
・Pr(A&cup;B)=Pr(A)+Pr(B)<br />
・2つの事象A, Bの結果が互い他に影響を及ぼさないとき、2つの事象A, Bは互いに独立であるといい、次の式が成り立つ。<br />
Pr(A&cap;B)=Pr(A)*Pr(B)<br />
<br />
◇条件付き確率<br />
&nbsp;2つの事象A, Bについて、[事象Aと事象Bが同時に起こる回数/事象Aが起こる回数]を、事象Aが起こったという条件のもとで事象Bが起こる条件付き確率といい、この条件付き確率をPr(B｜A)と表す。<br />
Pr(B｜A)=n(A&cap;B)/n(A)<br />
<br />
◇乗法公式<br />
&nbsp;2つの事象A, Bについて、<br />
Pr(A｜B)*Pr(B) = Pr(A&cap;B) = Pr(B｜A)*Pr(A)<br />
が成り立つ。この式を変形すれば以下のベイズの定理が得られる。<br />
Pr(B)=Pr(B｜A)*Pr(A)+Pr(B｜事象Aの余事象)*Pr(事象Aの余事象)<br />
Pr(A1｜B) = [Pr(B｜A1)*Pr(A1)]/[&Sigma;Pr(B｜Ai)*Pr(Ai)]<br />
つまり、事後確率（結果Bで原因A1となる）の確率を、<br />
条件付き確率（Pr(B｜Ai)）と事前確率（Pr(Ai)）を用いて上式から求めることができる。<br />
<br />
・ベイズ統計とニューラルネットワークは深く関係している。<br />
----------<br />
<br />
[1] 株式会社システム計画研究所「Pythonによる機械学習入門」オーム社<br />
[2] 斎藤康毅「ゼロから作るDeep Leaning」オライリー・ジャパン<br />
[3] 人工知能学会「深層学習」近代科学社<br />
[4] 石村貞夫「「超」入門ベイズ統計」ブルーバックス<br />
-----------------------------------------------------------------------<br />
◆ 手法<br />
<br />
◇決定木とは<br />
　決定木とはデータを複数のクラスに分類する教師あり学習の1つです。<br />
　決定木の学習では学習データから樹木モデルを生成します。何を基準に分岐を行うかで、決定木はいくつかの手法に分類できます。分岐後の集合の不純度に着目したCARTや、情報量（エントロピー）に基づいて決めるC4.5/C5.0などが知られています。<br />
　決定木のメリットは、分類ルールを樹木モデルとして可視化できるため、分類結果の解釈が比較的容易である点です。また、生成した分類ルールを編集することも可能です。<br />
　学習のための計算コストが低いという点もメリットとして挙げられます。ただし、決定木は過学習してしまう傾向があります。また、扱うデータの特性によっては樹木モデルを生成することが難しいケースもあります。<br />
<br />
◇アンサンブル学習<br />
　アンサンブル学習は、いくつかの性能の低い分類器（弱仮説器）を組み合わせて、性能の高い一つの分類器を作る手法です。弱仮説器のアルゴリズムは決まっていませんので、適宜選択する必要があります。<br />
　アンサンブル学習のイメージは、弱識別器の多数決です。「三人寄れば文殊の知恵」のように、凡人でも何人かで集まって相談すると良い結果を得られる、というイメージです。<br />
　アンサンブル学習は弱仮説器の生成方法によって２つに分類できます。<br />
・バギング<br />
　学習データを抜けや重複を許して複数個のグループに分割し、学習データのグループごとに弱仮説器を生成する手法です。分類時は、各弱仮説器の出力した分類結果の多数決を取ります。<br />
　複数のグループに分割した学習データグループに対して弱仮説器をそれぞれ生成します。つまり、一部の学習データグループに特化した弱仮説器を組み合わせて性能の良い分類器を作ることになります。<br />
・ブースティング<br />
　複数の弱仮説器を用意し、重み付きの多数決で分類を実現する方法です。<br />
　その重みも学習によって決定します。難易度の高い学習データを正しく分類できる弱仮説器の判別結果が重視されるように重みを更新していきます。ある弱仮説器が間違ったデータを難易度の高いデータとし、「難易度の高いデータの抽出」と「難易度の高いデータに特化した弱仮説器の重みの計算」を反復して弱仮説器の重みを決定します。<br />
　より難易度の高いデータを抽出しながら、難易度の高いデータを分類することに特化した分類器を順次生成していくイメージとなります。<br />
・Random Forest<br />
　アンサンブル学習のバギングに分類されるアルゴリズムです。学習データ全体の中から重複や欠落を許して複数個の学習データセットを抽出し、その一部の属性を使って決定木（弱仮説器）を生成します。学習、判別の処理が高速で、学習データのノイズにも強いというメリットがあります。また、分類だけでなく回帰やクラスタリングにも使えます。ただし、学習データ数が少ない場合は過学習となる傾向があります。学習データが少ない場合は、あまり利用しない方が良いでしょう。<br />
・AdaBoost<br />
　アンサンブル学習のブースティングに分類されるアルゴリズムの一つです。弱仮説器のアルゴリズムは決まっていないため、適宜選択する必要があります。難易度の高いデータを正しく分類できる弱仮説器の分類結果を重視するよう、弱仮説器に対して重みを付けます。難易度の高い学習データと性能の高い弱仮説器に重みを付けることで精度を上げます。<br />
　分類精度が高いですが、学習データのノイズに影響を受けやすい傾向があります。<br />
<br />
◇ サポートベクターマシン<br />
　サポートベクターマシン（Support Vector Machine）は、分類にも回帰にも使える優れた教師あり学習のアルゴリズムです。サポートベクタマシンがどのようなアイディアに基づくかを説明するために、まずデータを二つに分離する直線を引くことを例にして考えます。データを２つに分類する直線はいくつも選ぶことができますが、サポートベクターマシンでは分割線から最近傍サンプルデータまでのマージン（距離の２乗）の和を最大化する直線が一番良い分割線と考えます。サポートベクターマシンは学習データのノイズにも強く、分類性能のが非常に高いことが特徴です。他のアルゴリズムと比べると学習データ数もそれほど多く必要としません。ただし、分類処理速度は他のアルゴリズムと比べると遅くなります。また、基本的には２クラスの分類器となるため、多クラスの分類を行うためには複数のサポートベクターマシン分類器を組み合わせる必要があります。<br />
　サポートベクターマシンは優れた分類性能を持ちます。そのため、精度が要求される場合に利用されることが多いです。ただし、処理時間がかかる傾向があります。精度よりも処理速度の優先度が高い場合には、Random Forestのように軽量で高速なアルゴリズムを採用すると良いでしょう。<br />
<br />
◇ 回帰問題<br />
　回帰問題とは、数値を予測する問題です。学習時に入力データと出力データの組から対応する規則を学び、未知の入力データに対しても適切な出力を生成できるようにするものです。つまり、入力と出力の関係（関数）を推定し、近似する問題とも言えます。機械学習の観点では、正解データから学ぶので教師あり学習の一つと言えます。<br />
　回帰問題を解くことは、与えられたデータに対して関係を示す数式を仮定し、当該データに最も当てはまる数式の係数を決めていくことになります。<br />
<br />
◇ 式の形式での分類<br />
・線形回帰<br />
　直線的な関係を推定する回帰です。実はこれは誤解を招きやすい事例で、直線的な関係を表現する数式（１次式）だけとは限りません。線形回帰とは、<br />
Y = &omega;<sub>0</sub> + &omega;<sub>1</sub>x<sub>1</sub> + &omega;<sub>2</sub>x<sub>2</sub> + ... + &omega;<sub>p</sub>x<sub>p</sub><br />
のような式に対して、（&omega;<sub>0</sub>, ... , &omega;<sub>p</sub>）を求める問題として定式化されます。<br />
　ポイントは求めるべき対象が線形ということであり、式そのものが１次式であることを意味しない点です。実際、p=2, x<sub>1</sub>&rArr;x, x<sub>2</sub>&rArr;x<sup>2</sup>と置けば、Y = &omega;<sub>0</sub> + &omega;<sub>1</sub>x + &omega;<sub>2</sub>x<sup>2</sup>も線形回帰の対象になるので、線形回帰問題で解けそうな問題となります。<br />
・非線形回帰<br />
　線形以外のすべての回帰です。<br />
<br />
◇ 変数の数での分類<br />
・単回帰<br />
　単回帰とは入出力の関係が1変数（単一の変数）で成り立つ式（例: y = ax + b）を想定して解く回帰問題です。変数の数が問題ですので、式の次数が上がっても変数が１つであれば単回帰です。つまり単回帰にも線形/非線形の2つがあります。<br />
・重回帰<br />
　2変数以上を扱う回帰を重回帰といいます。（例：y = ax<sub>1</sub> + bx<sub>2</sub> + cx<sub>3</sub> + d）。<br />
　単回帰と同様に、線形/非線形の両方があります。<br />
<br />
◇ 回帰における評価<br />
　分類問題の場合は、正解/不正解がはっきりしますが、回帰は数値同士であるため同じ議論は使えません。回帰の場合の結果の妥当性を客観的に評価する指標としてR<sup>2</sup>決定係数が知られています。<br />
　一般に決定係数と呼ばれるものはいくつかありますが、R<sup>2</sup>決定係数は、以下で定義されます。<br />
　R<sup>2</sup> = 1 - （「観測値」と「予測値」の差の２乗和）/（「観測値」と「観測値全体の平均」の二乗和）<br />
観測された値は誤差を含むので正解とは異なります。観測値と予測値が真の値に近ければ、分子が0に近づき、R<sup>2</sup>決定係数は1に近くなります。値と観測値のズレが大きい場合、分子は0から離れ、したがって分数部分が0から離れて、結果としてR<sup>2</sup>決定係数は1から離れた値となります。つまり、R<sup>2</sup>決定係数の値が1に近いほど、その予測モデルは良いモデルであると言えます。<br />
<br />
◇ k-近傍法<br />
　k-近傍法（k-nearest neighbor）は、データの近傍性に基づく手法です。<br />
　未知のデータに対して、最も近いk個の学習データを選び出すことが基礎にあります。クラス分類で使われることが多いですが、値の平均を使うことで回帰問題にも適用できます。なお、クラスタリング問題でよく使われるk-means（k-平均法）は、名前は似ていますが異なる手法です。<br />
<br />
◇ クラスタリング<br />
　クラスタリングとは、データの性質からデータの塊（クラスタ）を作る手法です。対象が2次元であれば、散布図により大まかな構造を見られます。しかし対象が多次元である場合には想像がつきませんし、データが大量であれば人手で行うには無理があります。これを機械学習で実現するのがクラスタリングです。<br />
　クラスタリングには様々な手法が知られていますが、最もよく使われるのはk-meansです。k-meansは1950年代にはじめて論文の形で発表された手法です。比較的古い手法ですが、計算が簡単で直感的に分かり易いため、現代でもクラスタリング手法としてよく使われます。<br />
　クラスタリングは正解がなくとも実行できるので、データを探索しているときにも使用できます。クラスタリングの結果は、使用したクラスタリングアルゴリズムが見出した共通性による分類です。クラスタの意味付けは人が行わなければなりません。<br />
　クラスタリングは洞察のための道具と見ることもできます。<br />
<br />
◇ k-means<br />
　k-meansはクラスタ数を予め指定し、漸進的にクラスタ化を進める手法です。以下の手順でクラスタ化を行います。<br />
@各データを何らかの手段でクラスタに割り振ります。クラスタ中心を最初に決めて初期クラスタを形成する場合もあります。初期化手法はランダムでも構いませんが、後々の計算が効率的にできるk-means++法という手法もよく使われます。<br />
Aクラスタごとに中心を計算します。一般にはクラスタに属するデータ点の算術平均を用いることが多いでしょう。<br />
B各データからクラスタ中心への距離を求め、もしデータが最も近いクラスタ以外に属しているなら、データの所属を最も近いクラスタに変更します。<br />
C上記Bの手順でクラスタが変更になっていなければ、もしくは事前に決めたしきい値よりも変化量が小さければ処理を終了します。<br />
D新しいクラスタの割り振りを使って、Aから処理をやり直します。<br />
<br />
◇その他のクラスタリング手法<br />
　クラスタリングは大きく分けて、階層的（hierarchical）と非階層的（non-hierarchical）の２つに分類できます。<br />
・階層的クラスタリング<br />
　データを階層的に分類していく手法です。手法としてはさらに、トップダウン的に分割していく分枝（divisive）タイプとボトムアップ的に形成する凝集（agglomerative）タイプの２つに分かれます。実際には計算量の問題から凝集タイプが使われることが多いようです。<br />
　階層的凝集型クラスタリングはデータ１つがクラスタ１つに属する状態から始めて、距離の近いクラスタ同士を併合し凝集させることで全データが必要数のクラスタにまとまるまでクラスタリングを進めていく手法です。手法の特性から結果をデンドログラム（dendrogram/樹状図・系統樹）で表示できます。<br />
　階層的凝集型クラスタリングでは、切る位置によりクラスタ数を変更できます。凝集型クラスタリングの実現にはクラスタを凝集させるためのアルゴリズムが必要で、最短距離法、最長距離法、群平均法、ウォード法などいくつもの手法が知られています。<br />
・非階層的クラスタリング<br />
　クラスタ形成において階層を意識せず、評価関数を定義することにより当該評価関数が最適になるよう分割を実現する手法です。k-meansはこれにあたります。<br />
　Affinity propagationは、近年提案された非階層的クラスタリングの手法です。k-meansに比べて誤差が少ない、クラスタ数を予め決めておく必要がない（アルゴリズムが自動で決定します）、初期状態に依存しないなどのメリットがあります。一方、計算量が多く大規模計算が難しいというデメリットもあります。<br />
<br />
◇ データクレンジング<br />
　データの精査と洗浄。<br />
<br />
◇ 特徴量の導入<br />
　画像処理の分野では特定の特徴を取り出す特徴量の研究が進んでいます。<br />
　HOG（Histograms of Oriented Gradients）とは、画像を細かなセルに分解し、輝度の勾配方向をヒストグラム化したものです。HOG特徴量を使うことで物体の境界（エッジ）に関する情報を取り出しやすくなります。<br />
<br />
◇ パラメータチューニング<br />
　機械学習では、学習時の振る舞いを指定するための設定値を特に「ハイパーパラメータ」と呼び、最小二乗法の係数などの学習により変化するパラメータと区別しています。<br />
・グリッドサーチ<br />
　複数のハイパーパラメータの組み合わせを試し、最適な組み合わせを見つける作業を「グリッドサーチ」と呼びます。各パラメータに対して、いくつかの値を決めておき、すべての組み合わせを試して一番良いものを探すという手法です。<br />
<br />
◇ ファインチューニング<br />
　既に作られているネットワーク（VGG16など）を流用。<br />
　画像処理などでは、機械学習を用いて分類に適したフィルタ（特徴を抽出するためのフィルタ(3x3のマトリックスなど））などが作られており、それを流用することで、目的とする他の問題をより容易に解くことができるかもしれない。<br />
<br />
株式会社システム計画研究所「Pythonによる機械学習入門」オーム社<br />
-----------------------------------------------------------------------<br />
◆ 結果の評価手法<br />
<br />
◇ 混同行列（Confusion Matrix）
<table border="1" cellpadding="1" cellspacing="1" style="width: 433.72px;">
	<tbody>
		<tr>
			<td style="width: 37.72px; text-align: center;">&nbsp;</td>
			<td style="width: 72.72px; text-align: center;">&nbsp;</td>
			<td style="width: 139.72px; text-align: center;">予測</td>
			<td style="width: 164.72px; text-align: center;">予測</td>
		</tr>
		<tr>
			<td style="width: 37.72px; text-align: center;">&nbsp;</td>
			<td style="width: 72.72px; text-align: center;">&nbsp;</td>
			<td style="width: 139.72px; text-align: center;">Positive</td>
			<td style="width: 164.72px; text-align: center;">Negative</td>
		</tr>
		<tr>
			<td style="width: 37.72px; text-align: center;">実際</td>
			<td style="width: 72.72px; text-align: center;">Positive</td>
			<td style="width: 139.72px; text-align: center;">True Positive (TP)</td>
			<td style="width: 164.72px; text-align: center;">&nbsp;False&nbsp;Negative (FN)</td>
		</tr>
		<tr>
			<td style="width: 37.72px; text-align: center;">実際</td>
			<td style="width: 72.72px; text-align: center;">Negative</td>
			<td style="width: 139.72px; text-align: center;">False Positive (FP)</td>
			<td style="width: 164.72px; text-align: center;">True&nbsp;Negative (TN)</td>
		</tr>
	</tbody>
</table>
※ True（正解）、False（不正解）。行列内の名称は予測結果がTrueもしくはFalseであると読むと理解しやすい。<br />
<br />
◇ 正答率・適合率・再現率・F値<br />
・正答率（Accuracy）=(TP+TN)/(TP+FP+FN+TN)<br />
　全体の事象の中で正解がどれだけあったかの比率です。<br />
・適合率（Precision）=TP/(TP+FP)<br />
　分類器がPositiveと予測した中で、真にPositiveなものの比率です。<br />
　Positiveと予測したもののうち、どれぐらいが当たっているか、確度が高いかの指標です。Negativeラベルの適合率も定義可能です。適合率はラベル単位の指標となります。<br />
・再現率（Recall）=TP/(TP+FN)<br />
　真にPositiveなものに対して、分類器がどれだけPositiveと予測できたかを表します。<br />
　実際にPositiveなものの中から、どれぐらい検出できたかの指標と言うこともできます。<br />
　再現率もラベルごと指標であり、Negativeラベルの再現率も定義可能です。<br />
・F値（F-means）=2/(1/Precision + 1/Recall)=2Precision*Recall/(Presition + Recall)<br />
　これは適合率と再現率の調和平均です。２つの指標を総合的に見るときに便利です。<br />
　分類器の性能は大まかには正答率で見ますが、それだけでは不十分な場合があります。<br />
<br />
　ラベル間でデータ数が大きく異なる場合（例えばNegativeのテストデータがPositiveに比べてとても少ない場合）は、特に注意が必要です。また、適合率と再現率は異なる性質を持っています。<br />
　適合率と再現率はトレードオフの関係にありｍさう。分類器の誤検出（分類の誤り）を防ごうとすれば、予測の確度が高いものだけを拾い上げることになりますので、見落としの確率を増やすことになります（再現率低下）。一方、見逃しの割合を下げようとすれば、そのぶん外れる確率も増えます（適合率低下）。このように適合率と再現率は異なる意味合いがありますので、課題によってどちらを重視すべきか見極めなくてはなりません。場合によっては、これら２つの数値を見てそのラベルの総合的な分類性能を判断したいこともあるでしょう。F値はこうした場面で使用します。<br />
<br />
◇ホールドアウトと交差検証<br />
・ホールドアウト検証<br />
　対象データの中からテストデータを分離して、残りを学習データとする方法<br />
・k-分割交差検証<br />
　対象データをk個に分割し、そのうちの1個をテストデータ、残りのk-1個を学習データとして学習する方法。テストデータを変更しながらk回の学習と検証を繰り返す。このk回の検証結果を平均した結果を用いることが多いです。<br />
　分類器の性能評価において学習データとテストデータの分離は極めて重要です。実際には学習データがなかなか揃わず、量の確保が難しいということはよくあります。学習の観点からは学習データが多いに越したことはないですが、テストデータを確保しなければ評価ができない、でもテストデータを確保すると学習データが減ってしまい困る、といったジレンマも実務では発生しがちです。こうしたことに対応する１つの方法がk-分割交差検証です。<br />
　k-分割交差検証はkを大きくすれば良い性能が得られると期待できます。ただし、試行回数が増えてしまうので時間が掛かります。一方、kを小さくすると試行回数は減らせますが学習データが減ってしまいますので、あまりうれしくない事態です。学習・検証が素早く行えるのであればk-分割交差検証を行うのが良いでしょう。学習・検証に時間やコストがかかる場合はホールドアウトで検討をつけて、k-分割交差検証を少なくするという方法も考えられます。傾向をつかむ段階ではk-分割したとしても、学習・検証の回数を間引く場合もあります。<br />
　<br />
　ホールドアウト検証は手軽ですが、テストデータに偏りがないかの懸念がつきまといます。その点、交互とはいえ万遍なくテストデータを使用するk-分割交差検証はより良い評価ができます。ただしk回の学習と評価が必要ですので、そのぶん手間と計算時間がかかります。<br />
　汎化性能の確認が必要な場合は、k-分割交差検証を実施することをお勧めします。<br />
<br />
株式会社システム計画研究所「Pythonによる機械学習入門」オーム社<br />
-----------------------------------------------------------------------<br />
◇ 基礎的な特性に関するデータベース<br />
・COD (Crystallography Open Database): 結晶構造のデータ<br />
・AMCSD:&nbsp;結晶構造のデータ<br />
・Cambridge Crystallographic Data Center<br />
・Inorganic Crystal Structure Database (ICSD)<br />
・Citrination: 実験や計算のデータ<br />
・ESP: 電子構造<br />
・MRL Datamining Chart: 熱電特性<br />
・CCCBDB: 実験および計算での気相原子および小分子の熱化学データ<br />
・CRYSTMET (https://www.jaici.or.jp/wcas/wcas_crystmet.htm)<br />
・AiiDA<br />
・<a href="https://cmr.fysik.dtu.dk/#" style="color: rgb(27, 97, 214); font-family: &quot;nobile&quot;,sans-serif; font-size: 12.8px; font-style: normal; font-variant: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-decoration: none; text-indent: 0px; text-transform: none; -webkit-text-stroke-width: 0px; white-space: normal; word-spacing: 0px;">COMPUTATIONAL MATERIALS REPOSITORY</a><br />
・molecular space<br />
・SUNCAT<br />
<br />
◇ 第一原理計算によるデータベース<br />
・Material Project: VASPでのデータ（フォノンはAbinit, X線吸収はFEFF<br />
・AFLOW: VASPやQuantum Espressoのデータ<br />
・OQMD: VASPのデータ<br />
・NoMaD: 様々な第一原理計算コードのデータ<br />
・PubChemQC (http://pubchemqc.riken.jp/): HOMO-LUMO gap<br />
・TE Desing Lab: 熱電特性<br />
・Catalysis-Hub (https://www.catalysis-hub.org/): 触媒研究用<br />
・COMPUTATIONAL MATERIALS REPOSITORY (RMC)<br />
<br />
◇ 文献からのデータ抽出<br />
・ChemDataExtractor<br />
&nbsp; (https://pubs.acs.org/doi/10.1021/acs.jcim.6b00207)<br />
&nbsp; (https://pubs.acs.org/doi/10.1021/acs.chemrev.6b00851)<br />
<br />
◇ 記述子の例<br />
・fingerprint: NMR, IRに有効とされる<br />
・原子間の結合をグラフとして取り扱う方法<br />
&nbsp; (https://www.sciencedirect.com/science/article/pii/S1359644612002759)<br />
&nbsp; (https://pubs.acs.org/doi/abs/10.1021/ci400403w)<br />
・各原子の記述に周囲の原子の影響の重みを含めて取り扱う方法<br />
・原子配置の3次元構造情報を取り入れる方法<br />
&nbsp; (https://pubs.acs.org/doi/abs/10.1021/acs.jmedchem.7b00696)<br />
などがある。<br />
<br />
◇ 反応中の構造と安定性の変化をほぼ自動で調べ上げる方法<br />
・人工力誘起反応（Artificial Force Induced Reaction: AFIR）法<br />
　(https://pubs.rsc.org/en/content/articlelanding/2013/cp/c3cp44063j#!divAbstract)<br />
<br />
応用物理 第88巻 第10号 (2019)<br />
-------------------------------------------------------------<br />
■ 量子コンピュータ [R1]<br />
<br />
□ 特徴<br />
・量子力学の重ね合わせの原理を応用<br />
・量子ビットの同時計算による指数関数的な計算<br />
・28 qubits（qubits=quantum bit=キュービット）で従来のコンピュータ（古典コンピュータ）を超えるとされるが、エラーがあるため数百gubitsが求められる<br />
<br />
□ 応用<br />
・波動関数を求める<br />
&nbsp; アルゴリズムはまだ確定していない<br />
&nbsp; VQE法は古典と量子コンピュータを交互に用いる。古典コンピュータは波動関数で使われる&theta;の計算に使われる<br />
-------------------------------------------------------------<br />
■ MD-GAN [R1-R4]<br />
・Wasserstein GAN<br />
・質問では「マルコフ状態モデル」の話が出た。私の勝手な想像だが、MD-GANはマルコフ状態モデルに類似したモデルを作っているのではないかと思われた。論文[2]ではmarkovと検索したが該当する単語はないと表示された。<br />
※ 文献[3]では「確率論的時間発展としてモデル化」と書かれている<br />
-------------------------------------------------------------<br />
■ マーカス理論の式を再現しようとした場合 [R1]<br />
・エラーはデータ数が多くなると指数関数的に減少<br />
・記述子が良ければデータ数は40程度でよい<br />
・機械学習での予測値は物理や理論に基づいて記述子を書くと良くなる<br />
※ 記述子は原子の構造に関するものも入れることが必要で28個程度が必要になる。<br />
※&nbsp;記述子がまったく予測できなければ、化学分野では1000程度のデータでなかなか良い相関が得られるモデルを作ることができる傾向にある（それが難しい場合は最低数百（200〜400）以上はデータが欲しいところ）。<br />
※ 転移学習で利用する転移後のデータが40-50程度で上手くいっている論文を見ても納得できる結果。<br />
※ 余談ではあるが、著名な方々に尋ねると、CNNは中間層が3〜4層程度、RNNは10層程度が2019年での学会などでは一般的のようです。節の変化はピラミッド型に綺麗な形で減少していくというわけでもないので節の数については一般的にどのようになっていくか傾向はつかめていません。<br />
※ 中性子小角散乱（SANS）パターン（2次元データ）のその後の予測における真値との差（エラー）もデータ数に応じて指数関数的に減少していくので「エラーとデータ数の関係が指数関数的な傾向を示す」可能性がもしかしたら一般的にありえるかもしれない。<br />
-------------------------------------------------------------<br />
■ 統計数理 [R1]<br />
Q: どの位の訓練用データがあればどの位の正確な判断ができるのかを統計学的に示してください<br />
A: この要求に答えることは原理的にはできません。統計学的推論が導く帰結は、すべて特定の想定の下での条件付き命題であるからです。<br />
-------------------------------------------------------------<br />
■ PLRM [R6]<br />
・dPLRM-0: モデルを主にボルツマン分布と分配関数で記述し、カーネルを消すことができるように罰金項が加えられたもの。<br />
&nbsp; モデルの式からエネルギーに関連する事象を良い精度で計算できる。<br />
・DPLRM-2: CG法で解けるようにしたもの<br />
-------------------------------------------------------------<br />
■ 主成分分析などの多変量解析 [R1]<br />
・データ解析においてよく主成分分析などの多変量解析手法が用いられますが、変数間の関係が非線形でかつ正規分布などでは表現しきれない複雑な共起確率分布を持つ対象にはあまり役立ちません。<br />
※ 共起確率: 最小化問題では対数双対罰金付き尤度関数から、カーネルが得られ、相互作用が得られる<br />
-------------------------------------------------------------<br />
■ References<br />
<br />
[R1] 理研シンポジウム、2019<br />
[R2] Multi-Step Time Series Generator for Molecular Dynamics<br />
[R3] ディープラーニングによる分子シミュレーションデータの高効率化&nbsp;<br />
&nbsp; https://www.keio.ac.jp/ja/press-releases/files/2018/5/8/180508-1.pdf<br />
&nbsp; 確率論的時間発展としてモデル化と書かれている<br />
[R4] 機械学習による分子動力学シミュレーションの高速化<br />
（https://www.mext.go.jp/b_menu/shingi/chousa/shinkou/051/shiryo/__icsFiles/afieldfile/2019/01/31/1412851_3.pdf）<br />
[R5] MD-GAN<br />
&nbsp; https://github.com/jeiros/MD-GAN<br />
[R6] dPLRMを用いた話者識別<br />
&nbsp; https://www.ism.ac.jp/editsec/toukei/pdf/53-2-201.pdf<br />
-------------------------------------------------------------