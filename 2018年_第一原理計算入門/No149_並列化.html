&nbsp; ここでは並列化の方法について記述する。自作でPCクラスタを作る場合は、前園涼「自作PCクラスタ超入門 - ゼロからはじめる並列計算環境の構築と運用」森北出版株式会社、は必ず読んでおかねばならない。<br />
--------------------------------------------------------------------------------<br />
■ プログラミングにおける並列化<br />
----------<br />
◇ OpenMP<br />
　簡単な並列化であれば、ディレクティブを挿入するだけでよく、並列化オプションをつけない場合には、コメント行となるので、基本的にオリジナルの変更が不要な利点がある。<br />
<br />
・設定の仕方<br />
　下記のように、call 文の前に !$omp parallel、後に !$omp end parallel を記述する。呼び出すサブルーチンの中にある doループの前に !$omp parallel do を記述する。<br />
<br />
!$omp parallel<br />
call a(b, x, y, z)<br />
!$omp end parallel<br />
<br />
subroutine a(b, x, y, z)<br />
(省略)<br />
!$omp parallel do<br />
&nbsp; do 1=1, 100<br />
&nbsp;&nbsp;&nbsp; z(i) = b*x(i) + y<br />
&nbsp; end do<br />
&nbsp; return<br />
end<br />
----------<br />
◇ MPI<br />
　それぞれのノードで割り振られた部分だけを計算し、通信するデータ量も指定する。<br />
<br />
・設定の仕方<br />
　OpenMPやOpenACCと比べて記述は多くなるが、高い並列化を得られやすい。<br />
<br />
program mpi_sample<br />
implicit none<br />
include &#39;mpif.h&#39;<br />
integer:: IERR,MYRANK,NPROCS<br />
<br />
call MPI_INIT(IERR) ! MPI初期化<br />
call MPI_COMM_SIZE(MPI_COMM_WORLD,NPROCS,IERR) ! MPI 並列数を取得<br />
call MPI_COMM_RANK(MPI_COMM_WORLD,MYRANK,IERR) ! 自分のプロセス番号取得<br />
<br />
call para_range(1, n, nprocs, myrank, ista, iend)<br />
do i=ista, iend<br />
&nbsp; a(i)=a(i)+value<br />
end do<br />
<br />
call MPI_ALLREDUCE(a,a1,n_element,<br />
&nbsp; &amp; MPI_DOUBLE_PRECISION,<br />
&nbsp; &amp; MPI_SUM, MPI_COMM_WORLD, IERR)<br />
call MPI_FINALIZE(IERR) ! MPI 終了処理<br />
<br />
※ ista&nbsp;から iend までが各プロセッサに割り振られる。<br />
※ call MPI_ALLREDUCE で各プロセッサに割り振られて計算されたa(i)を纏める。まとめられたa(i)は各プロセッサに同じ値が渡される。<br />
References: <a href="http://www.cv.titech.ac.jp/~hiro-lab/study/mpi_reference/index.html">http://www.cv.titech.ac.jp/~hiro-lab/study/mpi_reference/index.html</a><br />
**********<br />
mpi_init 初期化<br />
mpi_comm_size 並列数取得<br />
mpi_comm_rank MPIランクID（0から並列数-1）を取得<br />
mpi_barrier 他のノードの進行を待って、ノード間で同期をとる<br />
mpi_bcast 一対全のブロードキャスト通信<br />
mpi_send ノード間（PC）の一対一データの送信<br />
mpi_recv ノード間（PC）の一対一データの受信<br />
mpi_reduce 全ノードのデータから一つのノードへ演算して還元する<br />
mpi_allreduce 全ノードのデータを全ノードへ演算して還元する<br />
mpi_finalize 終了<br />
※ mpi_reduceの場合<br />
call MPI_REDUCE(l,m,1,MPI_INTEGER,MPI_SUM,0<br />
&amp; ,MPI_COMM_WORLD,IERR) ! 各ノードの計算結果の和を0 番ノードに集める.<br />
**********<br />
----------<br />
◇ OpenACC (Open Accelerator)<br />
　これはGPUを用いたものである。OpenMPと似た記述になるようにしている。<br />
　開発環境として、コンパイラが出力するメッセージに従って、プログラムを改善していくことで、最善の結果が得られるようになっている。<br />
<br />
・ 設定の仕方<br />
　下記のように、doループの前に、!$acc parallel loop、doループの終わりであるend do の後に !$acc end parallel loop を記述する。<br />
<br />
!$acc parallel&nbsp;loop<br />
&nbsp; do 1=1, 100<br />
&nbsp;&nbsp;&nbsp; z(i) = b*x(i) + y<br />
&nbsp; end do<br />
!$acc end parallel loop<br />
&nbsp; return<br />
end<br />
----------<br />
[0] CMD25、講義資料<br />
[1] <a href="http://www.nvidia.co.jp/object/openacc-gpu-directives-jp.html">http://www.nvidia.co.jp/object/openacc-gpu-directives-jp.html</a><br />
[2] <a href="http://www.cms-initiative.jp/ja/events/2014-haishin">http://www.cms-initiative.jp/ja/events/2014-haishin</a><br />
※ 古典MD(classical MD)では専用機であるAntonを用いること！<br />
※ Intel のコンパイラには自動並列化のオプション -parallel が存在する。しかしながら、OpenMPやMPIによって並列化されているコードの方が計算速度が速いことをよく経験する。<br />
※ 余暇のある方は、MPIに挑戦してみるのも良いだろう。<br />
--------------------------------------------------------------------------------<br />
■ 並列化の評価方法<br />
◇ Amdahl&#39;s Law: [1]<br />
　Speed-up ratio = 1 / ( (1-P) + P/n )<br />
ここで、nはコア数、Pは並列化できる部分に対する並列化しないときの時間<br />
◇ Gustafson-Baris&#39;s Law: Amdahl&#39;s Lawよりも楽観的な見方。 [2]<br />
　Speed-up ratio &lt;= n + (1-n) * s<br />
ここで、nはコア数、sは逐次計算での比（並列化できない部分）<br />
[1] <a href="http://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%A0%E3%83%80%E3%83%BC%E3%83%AB%E3%81%AE%E6%B3%95%E5%89%87">http://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%A0%E3%83%80%E3%83%BC%E3%83%AB%E3%81%AE%E6%B3%95%E5%89%87</a><br />
[2] <a href="http://ja.wikipedia.org/wiki/%E3%82%B0%E3%82%B9%E3%82%BF%E3%83%95%E3%82%BD%E3%83%B3%E3%81%AE%E6%B3%95%E5%89%87">http://ja.wikipedia.org/wiki/%E3%82%B0%E3%82%B9%E3%82%BF%E3%83%95%E3%82%BD%E3%83%B3%E3%81%AE%E6%B3%95%E5%89%87</a><br />
--------------------------------------------------------------------------------<br />
■ 下記のHPを参考にすれば、並列化の方法が分かるようになる。<br />
実験化学者が作るPCクラスター： <a href="http://pccluster.web.fc2.com/index.html">http://pccluster.web.fc2.com/index.html</a>&nbsp;<br />
超並列計算研究会 : <a href="http://www.is.doshisha.ac.jp/SMPP/">http://www.is.doshisha.ac.jp/SMPP/</a><br />
理化学研究所での講習会 : <a href="http://accc.riken.jp/HPC/training.html">http://accc.riken.jp/HPC/training.html</a><br />
F. Kusuhara at Kubo and Shiohara Labo. :<br />
<a href="http://www.rcs.arch.t.u-tokyo.ac.jp/kusuhara/index-j.html">http://www.rcs.arch.t.u-tokyo.ac.jp/kusuhara/index-j.html</a>　<br />
並列計算機の使い方・設定の仕方（備忘録）:<br />
<a href="http://www2.kobe-u.ac.jp/~lerl2/calc_j.htm">http://www2.kobe-u.ac.jp/~lerl2/calc_j.htm</a><br />
<a href="http://blog.livedoor.jp/rootan2007/archives/51305070.html">http://blog.livedoor.jp/rootan2007/archives/51305070.html</a><br />
<br />
■ WIEN2kで他のPCとMPI並列をする場合<br />
　intel fortran compiler での オプションに　-i-static を入れて、PCに NFS を設定することがMPI並列を成功させる鍵となる。文献 [11 - 14] を参照するとNFSの設定方法が書かれている。<br />
<br />
参考文献<br />
[1] <a href="http://jpdb.nihs.go.jp/abinitmp/MPICH2%E7%92%B0%E5%A2%83%E3%81%AE%E6%A7%8B%E7%AF%89%20Windows%20Vista,%207%20%E7%B7%A8%EF%BC%8864bit%E7%92%B0%E5%A2%83%E5%AF%BE%E5%BF%9C%EF%BC%89%2020100819TN-0.pdf">http://jpdb.nihs.go.jp/abinitmp/MPICH2%E7%92%B0%E5%A2%83%E3%81%AE%E6%A7%8B%E7%AF%89%20Windows%20Vista,%207%20%E7%B7%A8%EF%BC%8864bit%E7%92%B0%E5%A2%83%E5%AF%BE%E5%BF%9C%EF%BC%89%2020100819TN-0.pdf</a><br />
[2] <a href="http://www2.itc.nagoya-u.ac.jp/pub/pdf/pdf/vol03_03/194_225kouza.pdf">http://www2.itc.nagoya-u.ac.jp/pub/pdf/pdf/vol03_03/194_225kouza.pdf</a>　<br />
[3] <a href="http://www.pasoble.jp/windows/7/08853.html">http://www.pasoble.jp/windows/7/08853.html</a><br />
[4] <a href="http://www.tigers-net.com/support/manual/win7_security/win7_firewall.html">http://www.tigers-net.com/support/manual/win7_security/win7_firewall.html</a><br />
[5] <a href="http://27bit.com/os-7-port.html">http://27bit.com/os-7-port.html</a><br />
[6] <a href="http://freesoft.tvbok.com/tips/win7rc64/login_administrator.html">http://freesoft.tvbok.com/tips/win7rc64/login_administrator.html</a><br />
[7] <a href="http://www.adminweb.jp/windows7/uac/index3.html">http://www.adminweb.jp/windows7/uac/index3.html</a><br />
[8] <a href="http://www.gigafree.net/faq/vista/">http://www.gigafree.net/faq/vista/</a><br />
[9] <a href="http://itpro.nikkeibp.co.jp/article/COLUMN/20100914/352030/">http://itpro.nikkeibp.co.jp/article/COLUMN/20100914/352030/</a><br />
[10] <a href="http://www.jamstec.go.jp/esc/simschool/f90learning/">http://www.jamstec.go.jp/esc/simschool/f90learning/</a><br />
[11] <a href="http://www.atmarkit.co.jp/flinux/rensai/linuxtips/367usenfs.html">http://www.atmarkit.co.jp/flinux/rensai/linuxtips/367usenfs.html</a><br />
[12] <a href="http://kajuhome.com/nfs.shtml">http://kajuhome.com/nfs.shtml</a><br />
[13] <a href="http://mountainbigroad.jp/fc5/nfs.html">http://mountainbigroad.jp/fc5/nfs.html</a><br />
[14] <a href="http://unixlife.jp/unixlife/linux/s-nfs.jsp">http://unixlife.jp/unixlife/linux/s-nfs.jsp</a><br />
-------------------------------------------------------------------------------<br />
■ LAM&nbsp;<br />
1) <a href="http://www.lam-mpi.org/">http://www.lam-mpi.org/</a>, <a href="http://www.lam-mpi.org/7.1/download.php">http://www.lam-mpi.org/7.1/download.php</a><br />
2) tar zxvf lam-7.1.4.tar.gz<br />
3) cd lam-7.1.4<br />
4) ./configure CC=icc CXX=icpc FC=ifort --prefix=/usr/lib/lam<br />
5) make<br />
6) sudo make install<br />
--------------------------------------------------------------------------------<br />
■ OpenMPI<br />
1) <a href="http://www.open-mpi.org/">http://www.open-mpi.org/</a><br />
2) tar zxvf openmpi-1.4.3.tar.gz<br />
3) cd openmpi-1.4.3<br />
4) sudo su<br />
5) ./configure -prefix=/usr/local/openmpi CXX=icpc CC=icc FC=ifort F90=ifort F77=ifort<br />
6) make<br />
7) make install<br />
--------------------------------------------------------------------------------<br />
■ MPICH<br />
1) tar -zxvf mpich-1.2.7p1.tar.gz<br />
2) cd mpich-1.2.7p1<br />
3) sudo su<br />
4) ./configure -prefix=/usr/local/mpich -opt=-O2 -c++=icpc -cc=icc -fc=ifort -f90=ifort -f77=ifort --with-arch=LINUX<br />
5) make<br />
6) make install<br />
7) /usr/local/mpich/share/machines.LINUX にMPIで利用するマシン名の一覧を記入<br />
8) 上記のprefixにした場合は、/usr/local/mpich/bin/mpif90 でコンパイルが可能になる<br />
--------------------------------------------------------------------------------<br />
■ 並列計算を実行する前に必要なrshの設定（CentOS の場合）<br />
1) 「System」&rarr;「Administration」&rarr;「Add/Remove Software」<br />
2) 左上にある検索の欄で、「rsh-server」を入力して対応するソフトを探す。<br />
3) Querying が消えるまで待つ。<br />
4) 右側にインストールするソフトの候補が現れるので、インストールしたいrshと書かれたものにチェックを入れて、「Apply」を押してインストールする。<br />
5) rpm -qa rsh-server, rpm -qa xinetd<br />
&nbsp; rsh-server, eintedがインストールされていることを確認する<br />
6) su<br />
7) /sbin/chkconfig rsh on<br />
&nbsp; ブート時に rsh を自動的に起動するように設定する<br />
8) /sbin/chkconfig rlogin on<br />
&nbsp; ブート時にrloginを自動的に起動するように設定する<br />
9) /sbin/service xinetd restart<br />
&nbsp; ブート時にxinetd を自動的に起動するように設定する<br />
10) ホームディレクトリに .rhosts ファイルへ通信するマシン全てと localhost.localdomain を記述<br />
&nbsp;&nbsp;&nbsp; kwrite ~/.rhosts<br />
&nbsp;&nbsp;&nbsp; localhost.localdomain<br />
&nbsp;&nbsp;&nbsp;&nbsp;host1<br />
&nbsp;&nbsp;&nbsp; host2<br />
&nbsp;&nbsp;&nbsp; host3<br />
11) .rhosts のパーミッションを600にする。chmod 600 ~/.rhosts<br />
12) rsh の動作確認<br />
&nbsp;&nbsp;&nbsp; rsh localhost.localdomain hostname<br />
&nbsp;&nbsp;&nbsp; rsh&nbsp;host1 hostname<br />
参考HP : <a href="http://mase.itc.nagoya-u.ac.jp/~hirano/pukiwiki/index.php?CentOS5%A4%C7rsh%A4%F2%CD%AD%B8%FA%A4%CB%A4%B9%A4%EB">http://mase.itc.nagoya-u.ac.jp/~hirano/pukiwiki/index.php?CentOS5%A4%C7rsh%A4%F2%CD%AD%B8%FA%A4%CB%A4%B9%A4%EB</a><br />
--------------------------------------------------------------------------------<br />
■ ベンチマーク (Abinit)<br />
　下記の結果から分かるように、4コア(8スレッド）のCPUを用いた場合には、MPI並列のみでのmpirun -np 4 がもっとも速い。OpenMPのみを用いる場合には、export OMP_NUM_THREADS=4 がもっとも速い。そして、MPI並列 &gt; OpenMP並列となっている。<br />
　ここには記していないが、gfortranではなく、intel fortran を用いると計算速度が1-3割向上。<br />
　Intel&reg; Core&trade; i7-3820 CPU @ 3.60GHz &times; 8　は Intel&reg; Core&trade; i7-2700K CPU @ 3.50GHz &times; 8&nbsp;より計算速度が2割程度向上する。クロック数の向上は僅かのため、L3キャッシュが効いているかもしれない。<br />
<br />
PWscf: <a href="http://qe-forge.org/gf/download/frsrelease/116/403/espresso-5.0.2.tar.gz"><font color="#0000ff">espresso-5.0.2.tar.gz</font></a><br />
PWgui: <a href="http://www-k3.ijs.si/kokalj/pwgui/download/pwgui-5.0.2-linux-x86_64.tgz"><font color="#0000ff">pwgui-5.0-linux-x86_64.tgz</font></a><br />
PAW: http://theossrv1.epfl.ch/Main/Pseudopotentials -&gt; <a class="urllink" href="http://theossrv1.epfl.ch/uploads/Main/NoBackup/pbe.0.3.1.tgz" rel="nofollow"><font color="#0000ff">pbe.0.3.1.tgz</font></a><br />
<br />
Abinit: <a href="http://www.abinit.org/downloads/source-packages/abinit-1/releases/7.6.4"><font color="#0000ff">ABINIT 7.6.4</font></a><br />
PAW: http://www.abinit.org/downloads/PAW2/JTH-TABLE/index.html -&gt; http://www.abinit.org/downloads/PAW2/JTH-TABLE/index.html<br />
<br />
■ Abinit test case<br />
http://www.abinit.org/downloads/PAW2/JTH-TABLE/index.html -&gt; C.GGA_PBE-JTH-paw.xml<br />
<br />
tpaw1_1.files<br />
----------<br />
tpaw1_1.in<br />
tpaw1_1.out<br />
tpaw1_1i<br />
tpaw1_1o<br />
tpaw1_1tmp<br />
./C.GGA_PBE-JTH-paw.xml<br />
----------<br />
<br />
tpaw1_2.files (リリース 12.04 (precise) 64-bit, Intel&reg; Core&trade; i7-2700K CPU @ 3.50GHz &times; 8 )<br />
◆ OpenMPI + OpenMP (gfrotran) case<br />
highest speed: export OMP_NUM_THREADS=1 + mpirun -np 4 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
<br />
Resutls<br />
------------------<br />
export OMP_NUM_THREADS=1<br />
mpirun -np 1 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp; &nbsp;Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 16.5&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 16.6<br />
mpirun -np 2 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.1&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.1<br />
mpirun -np 2 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11.0&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11.1<br />
mpirun -np 4 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10.7&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10.7<br />
mpirun -np 5 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.6&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.6<br />
------------------<br />
export OMP_NUM_THREADS=2<br />
mpirun -np 1 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 21.3&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 13.8<br />
mpirun -np 2 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 15.8&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11.2<br />
mpirun -np 3 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 15.3&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 11.1<br />
mpirun -np 4 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 15.6&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.2<br />
------------------<br />
export OMP_NUM_THREADS=3<br />
mpirun -np 1 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 26.5&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 13.2<br />
------------------<br />
export OMP_NUM_THREADS=4<br />
mpirun -np 1 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 32.4&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.9<br />
mpirun -np 2 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 27.9&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12.9<br />
------------------<br />
export OMP_NUM_THREADS=6<br />
mpirun -np 1 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 50.8&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 13.9<br />
------------------<br />
export OMP_NUM_THREADS=8<br />
mpirun -np 1 /usr/local/bin/abinit &lt;tpaw1_2.files<br />
&nbsp;&nbsp; Proc.&nbsp;&nbsp; 0 individual time (sec): cpu=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 91.5&nbsp; wall=&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 18.3<br />
------------------<br />
ngkpt 6 6 6<br />
nshiftk 4<br />
shiftk 0.5 0.5 0.5<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.5 0.0 0.0<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0 0.5 0.0<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.0 0.0 0.5<br />
------------------<br />
<br />
QE-ABINIT<br />
0. http://qe-forge.org/gf/project/q-e/frs/?action=FrsReleaseBrowse&amp;frs_package_id=32<br />
&nbsp; or http://qe-forge.org/gf/project/q-e/frs/?action=FrsReleaseBrowse&amp;frs_package_id=32<br />
1. tar zxvf QE-ABINIT.tar.gz<br />
2. cd QE*<br />
3. patch -p0 -d &lt; QE-ABINIT-diff<br />
-----------------------------------------------------------------------------<br />
■ ベンチマーク (Elk)<br />
-----<br />
http://sourceforge.net/p/elk/discussion/897820/thread/10a7ccbf/
<p>I tested using a standard desktop PC with a Intel Core i5-2500 3.30GHz running Ubuntu 12.04 x64. I used the standard compiler options (-O3 -ip -unroll -no-prec-div -openmp) for ifort and (-O3 -ffast-math -funroll-loops -fopenmp) for gfortran. With ifort I tested both the Elk included BLAS/LAPACK libraries and the Intel MKL BLAS/LAPACK libraries. With gfortran only the Elk libraries.</p>

<p>The times are as reported by the makefile modification (repeatable to within 0.5 sec)</p>

<p>Elk v2.2.10<br />
gfortran (v4.6.3) Elk libraries 154.7 sec<br />
ifort (v14.0.1) Elk libraries 131.5 sec (note: see other post)<br />
ifort (v14.0.1) Intel MKL libraries 126.6 sec</p>

<p>I also tested Elk(v2.2.1) to see how the code has improved (note: tests may have changed slightly)</p>

<p>Elk v2.2.1<br />
gfortran (v4.6.3) Elk libraries 210.1 sec<br />
ifort (v14.0.1) Intel MKL libraries 146.6 sec</p>
-----<br />
http://sourceforge.net/p/elk/discussion/897820/thread/10a7ccbf/
<p>I have done a little additional testing of FFT libraries. Elk includes support for 3 FFT libraries. The built in FFTPACK, FFTW3 and Intel MKL. I have tested using Intel MKL BLAS/LAPACK libraries (v11.1.1) with dynamic linking.</p>

<p>I built them all using ifort (v14.0.1) with -O3 -ip -unroll -no-prec-div -openmp -xhost -mkl=sequential</p>

<p>The results:<br />
Elk FFTPACK 121.6 sec<br />
FFTW (v3.3.3) 114.6 sec<br />
Intel MKL FFT 114.7 sec<br />
-----<br />
http://sourceforge.net/p/elk/discussion/897820/thread/10a7ccbf/</p>

<div class="markdown_content">
<p>A quick update from the gfortran/openblas side:<br />
1. The bug in the haswell-enabled development version of openblas has been fixed - turned out they had inadvertently disabled openmp support in their copy of LAPACK.<br />
2. For gfortran, one should add at least -mtune=native to the list of compiler flags to avoid trading performance for portability. For my tests, I ended up with the full (and possibly somewhat redundant) list of<br />
-O3 -ffast-math -funroll-loops --param max-unroll-times=2 -fopenmp -march=native -mtune=native -mavx -msse -msse2 -msse3 -msse4 -msse4.1 -msse4.2 -fprefetch-loop-arrays -fvariable-expansion-in-unroller -mfpmath=sse<br />
snarfed from a discussion of compiler benchmarks on the gcc-devel mailing list.</p>

<p>On an i7-4770K@3.4GHz,gcc-4.8.1,openblas-0.2.9.rc and setting OMP_NUM_THREADS=4 to avoid blocking on the HT pseudocores, the tests now complete in 122.6 seconds. I will try with FFTW next.</p>
</div>

<p>-----------------------------------------------------------------------------</p>
